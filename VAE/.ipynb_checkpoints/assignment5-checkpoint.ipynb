{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction and Generative Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational autoencoder (VAE, [Kingma and Welling,2013](https://arxiv.org/abs/1312.6114)) is a generative model. Like an autoencoder, the VAE projects input data into a latent space and then tries to reconstruct the original data back from the latent space representation. But instead of mapping the data to a single point in the embedding space (now we will refer to this as the *latent space*) it maps it to a distribution. \n",
    "\n",
    "In VAE, we map each datapoint to a mean and standard deviation of a multivariate Gaussian (the number of dimensions of this Gaussian is the dimension of our latent space). Now, to get a re-construction, we sample from the latent distribution and project the sample back into the original data space (to the space of MNIST images, in our case). \n",
    "\n",
    "The architecture of a simple VAE model to be implemented as part of this exercise is shown below. Blocks used to construct the cost function are shown in blue color. (Refer to this [tutorial](https://arxiv.org/abs/1606.05908) for more details) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='vae.png',height=150,width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the neural network methods we have seen previously, VAE takes Variational Inference approach for learning from the data. As shown during the lecture, using Variational Inference approach, for an observation matrix $\\mathbf{X}$ and corresponding latent space representation $\\mathbf{z}$, we can arrive at a lower bound for marginal likelihood as :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\begin{split}\n",
    "        \\log p(\\mathbf{X}) &\\ge \\mathbb{E}_{\\mathbf{z} \\sim q(\\mathbf{z}|\\mathbf{X})}\\left[\\log p(\\mathbf{X}|f(\\mathbf{z}))\\right] \\: - KL\\left[q(\\mathbf{z|X}) || p(\\mathbf{z}) \\right] \\\\\n",
    "\\text{}\\\\\n",
    "\\text{Where,} \\\\\n",
    "        q(\\mathbf{z|X}) &= \\mathcal{N}(\\mathbf{z}|\\mu(\\mathbf{X}),\\Sigma(\\mathbf{X})) \\:\\:\\: \\text{is the variational posterior distribution}\\\\\n",
    "        p(\\mathbf{z}) &= \\mathcal{N}(\\mathbf{z}|\\textbf{0},\\textbf{I}) \\:\\:\\: \\text{is the prior distribution}\\\\\n",
    "        \\\\\n",
    "        p(\\mathbf{X}|f(\\mathbf{z})) &: \\text{likelihood function}\\\\ \n",
    "        \\\\\n",
    "        \\mu(\\mathbf{X}) &: \\text{a function mapping from data space to latent space}\\\\\n",
    "        \\Sigma(\\mathbf{X}) &: \\text{a function mapping from data space to latent space}\\\\\n",
    "        f(\\mathbf{z}) &: \\text{a function mapping from latent space to data space}\n",
    "\\end{split}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By maximizing the above lower bound on the marginal likelihood, the model tries to learn parameters of the variational distribution (encoder mapping through a feed-forward network) and the reconstruction (decoder mapping again as a feed-forward network). \n",
    "\n",
    "So the lower bound can be seen as a sum of two terms:\n",
    "  1. The reconstruction term \n",
    "  2. The KL divergence term \n",
    "  \n",
    "The intuition behind the first term can thought of learning the data fit, that is, given $\\mathbf{z}$ the model learns a mapping to match the given data. However, the expectation part cannot be analytically solved, instead, we take samples from $q(\\mathbf{z|X})$ and consider average of $p(\\mathbf{X}|f(\\mathbf{z}))$ over these samples as an approximation for the expectation. (in this exercise we will be taking only one sample)\n",
    "\n",
    "The second term, the KL term, tries to penalize deviation of variational distribution from the prior. \n",
    "(we could still train the model without the KL part, would there be any issues then?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 01. KL divergence derivation (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference in the cost function between normal autoencoder and VAE is the KL term. The following section deals with deriving this term for the model under consideration, and understanding the intuition behind it. \n",
    "\n",
    "Kullback-Leibler divergence can be interpreted as a measure of \"distance\" between two distributions. However, it is not a metric, that is, $\\operatorname{D_{KL}}(q||p) \\ne \\operatorname{D_{KL}}(p||q)$. Read more in this [post](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) for a good introduction to KL divergence.\n",
    "\n",
    "Formally it is defined as below:\n",
    "$$\n",
    "    \\begin{split}\n",
    "        \\operatorname{D_{KL}}(q(x)||p(x)) &= \\int q(x) \\log \\frac{q(x)}{p(x)} dx\\\\\n",
    "        &= \\underbrace{\\int q(x) \\log q(x) dx}_{\\text{negative entropy}} - \\underbrace{\\int q(x) \\log p(x) dx}_{\\text{cross-entropy}}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current model, second term in the lower bound defines the KL divergence between two diagonal multivariate Gaussians : a prior distribution $p(\\mathbf{z})$ and a variational posterior $q(\\mathbf{z})$ (i.e. latent dimensions are assumed to be independent).\n",
    "$$\n",
    "    \\begin{split}\n",
    "        \\operatorname{D_{KL}}(q(\\mathbf{z|X})||p(\\mathbf{z})) &= \\underbrace{\\int q(\\mathbf{z|X}) \\log q(\\mathbf{z|X}) d\\mathbf{z}}_{\\text{A}} - \\underbrace{\\int q(\\mathbf{z|X}) \\log p(\\mathbf{z}) d\\mathbf{z}}_{\\text{B}}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The derivation of the entropy part is given below, and your task would be to complete the derivation for the cross-entropy part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy derivation:**<br>\n",
    "Let $q(\\mathbf{z|X})$ be a $J$ dimensional Gaussian random variable with mean $\\mu(\\mathbf{X})$ and diagonal covariance $\\Sigma(\\mathbf{X)}$. In order to simplify notations, lets denote them as $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ respectively. <br>\n",
    "Because of the diagonal covariance structure, the density can be factorized as below.\n",
    "$$\n",
    "\\begin{split}\n",
    "q(\\mathbf{z}) &= \\prod_{j=1}^J \\mathcal{N}\\left(z_j \\:|\\: \\mu_j,\\Sigma_{(j,j)}\\right)\\\\\n",
    "&= \\prod_{j=1}^J \\mathcal{N}(z_j | \\mu_j,\\sigma_j^2)\n",
    "\\end{split}\n",
    "$$\n",
    "Where $\\mathcal{N}(z_j | \\mu_j,\\sigma_j^2)$ represents a univariate Gaussian with mean $\\mu_j$ and variance $\\sigma_j^2$ respectively.\n",
    "\n",
    "The entropy term can thus be written as:\n",
    "$$\n",
    "    \\begin{split}\n",
    "        A &= \\int \\mathcal{N}(\\mathbf{z}\\:|\\:\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) \\log \\mathcal{N}(\\mathbf{z}\\:|\\:\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) \\: d\\mathbf{z}\\\\\n",
    "        &= \\int \\prod_{j=1}^J \\mathcal{N}(z_j | \\mu_j,\\sigma_j^2) \\: \\log \\prod_{j=1}^J \\mathcal{N}(z_j | \\mu_j,\\sigma_j^2) \\: d z_j\\\\\n",
    "        &= \\sum_{j=1}^J \\int \\mathcal{N}(z_j | \\mu_j,\\sigma_j^2) \\: \\log \\mathcal{N}(z_j | \\mu_j,\\sigma_j^2) \\: d z_j \\\\\n",
    "        &= \\sum_{j=1}^J \\mathbb{E}_{z_j \\sim \\mathcal{N}(\\mu_j,\\sigma_j^2)} \\: \\log \\left( \\frac{1}{\\sqrt{2\\pi \\sigma_j^2}} e ^{- \\frac{(z_j - \\mu_j)^2}{2 \\sigma_j^2} } \\right) \\\\\n",
    "        &= \\sum_{j=1}^J - \\frac{1}{2} \\left[ \\log 2\\pi + 2 \\log \\sigma_j + \\frac{1}{\\sigma_j^2} \\mathbb{E}_{z_j \\sim \\mathcal{N}(\\mu_j,\\sigma_j^2)} (z_j - \\mu_j)^2 \\right] \\\\\n",
    "        &= \\sum_{j=1}^J - \\frac{1}{2} \\left[ \\log 2\\pi + 2 \\log \\sigma_j + \\frac{1}{\\sigma_j^2} \\left(\\sigma_j^2\\right) \\right]\\\\\n",
    "        &= \\sum_{j=1}^J - \\frac{1}{2} \\left[ \\log 2\\pi + 2 \\log \\sigma_j + 1 \\right]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Where we used the fact that expectation is a linear operator and $\\mathbb{E}_{x \\sim \\mathcal{N}(\\mu,\\sigma^2)} [\\: (x-\\mu)^2 \\:] = \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-entropy derivation:**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************\n",
    "$$\n",
    "\\begin{split}\n",
    "    B &=\\int q(\\mathbf{z|X}) \\log p(\\mathbf{z}) d\\mathbf{z}\\\\\n",
    "    &= \\int \\mathcal{N}(\\mathbf{z}\\:|\\:\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) \\log \\mathcal{N}(\\mathbf{z}\\:|\\:\\boldsymbol{0},\\boldsymbol{I}) \\: d\\mathbf{z}\\\\\n",
    "    &= \\sum_{j=1}^J \\int \\mathcal{N}(z_j | \\mu_j,\\sigma_j^2) \\: \\log \\mathcal{N}(z_j |0,1) \\: d z_j \\\\\n",
    "        &= \\sum_{j=1}^J \\int \\mathcal{N}(z_j | \\mu_j,\\sigma_j^2) \\: \\log \\left( \\frac{1}{\\sqrt{2\\pi}} e ^{- \\frac{z_j^2}{2} } \\right) \\: d z_j \\\\\n",
    "        &= \\sum_{j=1}^J \\int \\mathcal{N}(z_j | \\mu_j,\\sigma_j^2) \\:\\big( -\\frac{1}{2}\\log 2\\pi \\:+ - \\frac{z_j^2}{2}\\big) \\: d z_j \\\\\n",
    "        &= -\\frac{1}{2}\\sum_{j=1}^J \\:\\big( \\log 2\\pi \\:+ \\mathbb{E}_{z_j \\sim \\mathcal{N}(\\mu_j,\\sigma_j^2)} z_j^2\\big) \\: d z_j \\\\\n",
    "        &= -\\frac{1}{2}\\sum_{j=1}^J \\:\\big( \\log 2\\pi \\:+ \\mu^2 + \\sigma^2 \\big)\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Where we used the fact that expectation is a linear operator and $\\mathbb{E}_{x \\sim \\mathcal{N}(\\mu,\\sigma^2)} [\\: x^2 \\:] = \\mathbf{Var}_{x \\sim \\mathcal{N}(\\mu,\\sigma^2)} [\\: x \\:] + \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu,\\sigma^2)} [\\: x \\:]^2 =\\sigma^2 +\\mu^2$\n",
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combined KL:**\n",
    "\n",
    "Combining above results for entropy and cross-entropy terms, the KL term can be written as :\n",
    "$$\n",
    "    \\begin{split}\n",
    "        \\operatorname{D_{KL}}(q(\\mathbf{z|X})||p(\\mathbf{z})) &= A - B \\\\\n",
    "        &= \\sum_{j=1}^J -\\frac{1}{2}\\left[2 \\log \\sigma_j + 1 - (\\mu_j^2 + \\sigma_j^2)  \\right]\\\\\n",
    "        &= -\\frac{1}{2} \\sum_{j=1}^J \\left[1+ \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2)  \\right]\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final lower bound**:\n",
    "\n",
    "In this exercise, since we consider MNIST digits as binary variables, the likelihood distribution $p(\\mathbf{x}|f(\\mathbf{z}))$ is assumed to be Bernoulli distribution. We also assume that all the data dimensions are independent. Thus the complete log likelihood with Bernoulli observations can be written as:\n",
    "$$\n",
    "\\begin{split}\n",
    "     \\log p(\\mathbf{X}|f(\\mathbf{z})) &= \\log \\prod_{n=1}^{N} \\prod_{d=1}^{D} p(x_{(n,d)}|f(z)_{(n,d)})\\\\\n",
    "     &= \\sum_{n=1}^{N}\\sum_{d=1}^{D} \\log \\: p(x_{(n,d)}|f(z)_{(n,d)})\\\\\n",
    "     &= \\sum_{n=1}^{N}\\sum_{d=1}^{D} \\log \\: f(z)_{(n,d)}^{x_{(n,d)}} + (1-f(z)_{(n,d)})^{(1-x_{(n,d)})}\\\\\n",
    "     &= \\sum_{n=1}^{N}\\sum_{d=1}^{D} ( x_{(n,d)} \\cdot \\log f(z)_{(n,d)}) + ((1-x_{(n,d)}) \\cdot \\log (1-f(z)_{(n,d)}))\n",
    "\\end{split}\n",
    "$$\n",
    "Where $N$ is the number of observations, $J$ is the number of latent dimensions, $D$ is the number of dimensions for each observation. Indexes along these dimensions are represented by lower case letters.<br>\n",
    "\n",
    "\n",
    "Earlier, we had derived analytical form of KL divergence for the model under consideration (for each observation). Finally, by combining the likelihood and KL part across all the observations, we can arrive at the lower bound as:\n",
    "$$\n",
    "\\begin{split}\n",
    "     \\log p(\\mathbf{X}) &\\ge \\sum_{n=1}^{N} \\left\\{ \\sum_{d=1}^D \\mathbb{E}_{\\mathbf{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu(\\mathbf{x}_{(n:)})},\\Sigma(\\mathbf{x}_{(n:)})} \\left[ \\log p(x_{(n,d)}|f(z)_{(n,d)}) \\right] \\: + \\: \\sum_{j=1}^J \\frac{1}{2} \\left[1+ \\log \\sigma_{(n,j)}^2 - \\mu_{(n,j)}^2 - \\sigma_{(n,j)}^2)  \\right] \\right\\}\\\\\n",
    "\\text{}\\\\\n",
    "\\text{Where,}\\\\\n",
    "    &\\mu_{(n,j)}, \\sigma_{(n,j)}^2 \\: \\text{are mean and variance of $j^{th}$ latent dimension for $n^{th}$ observation}\\\\\n",
    "    &\\mathbf{X} \\text{ represents observation matrix, $\\mathbf{x}_{(n:)}$ the $n^{th}$ observation vector and $x_{(n,d)}$ the corresponding scalar value on $d^{th}$ dimension}\n",
    "    \\end{split}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drawing samples from the VAE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the VAE is a generative model, so we must be able to draw samples from it. Minimizing the 'overlap' term in the VAE cost allows us to approximate a complex posterior to a diagonal multivariate normal. After training the VAE, we should then be able to sample from the prior and project them to data space to get samples of (in this case) MNIST images. So, we have a generative model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='vae_sampling.png',height=100,width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Implementation (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an intuitive understanding of the VAE, let's implement it. Note that VAE takes quite a lot of iteration to get trained with computational resources accessible for the assignments (over 500 epochs on full training dataset with 500 mini-batch size). \n",
    "\n",
    "The expectation from the home assignment is to understand and implement generative models, not to train the model completely. For implementation, choose the training size, mini-batch size and training parameters such that computation resources can be effectively utilized. Training size of 5k observations with mini-batch size = 50 and Adam learning rate = 0.001 might be good values to begin with.\n",
    "\n",
    "After successful completion of the implementation, run the model for at least 20 epochs and plot mean training and mean validation costs across iterations. \n",
    "\n",
    "This part of the assignment will carry 2 points. A correct implementation will be the one for which mean cost goes down to $\\approx$ 200 for reasonable hyperparameter initializations.\n",
    "\n",
    "Note: Retain the symbolic variable names as given below. For the discussion part, pre-trained weights will be given and it needs all the Theano variables named as given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Libraries and dataset imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# theano imports\n",
    "import theano\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "# plotting functionalities\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# load pretained objects\n",
    "import six.moves.cPickle as pickle\n",
    "\n",
    "# helper functions\n",
    "from exercise_helper import load_data,gradient_updates_Adam\n",
    "from exercise_helper import plot_2d_proj,plot_act_rec,plot_latent_space2D\n",
    "from exercise_helper import plot_img, plot_activations\n",
    "\n",
    "theano.config.floatX = \"float32\"\n",
    "\n",
    "# random number generators\n",
    "rng_np = np.random.RandomState(23455)\n",
    "srng = RandomStreams(seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Loading data *****\n"
     ]
    }
   ],
   "source": [
    "# change the train_size to take subsets of the data\n",
    "datasets = load_data('mnist.pkl.gz', train_size = 5000,binarize=\"stochastic\")\n",
    "train_x, train_y = datasets[0]\n",
    "valid_x, valid_y = datasets[1]\n",
    "test_x, test_y   = datasets[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implementing the VAE in theano\n",
    "# similar network architecture to (Kingma and Welling 2013)\n",
    "\n",
    "# symbolic variable declaration\n",
    "input = T.matrix(\"input\") # observation matrix as input \n",
    "learning_rate = T.scalar(\"learning_rate\") # learning rate for the optimizer\n",
    "mbidxs = T.lvector(\"mbidxs\") # mini-batch index\n",
    "\n",
    "\n",
    "\n",
    "###################################################################\n",
    "###################### INSERT YOUR CODE HERE ######################\n",
    "###################################################################\n",
    "\n",
    "# define number of latent dimensions \n",
    "# use 2 latent dimensions for the implementation\n",
    "latent_dim = 2\n",
    "# a list of layer dimensions\n",
    "dims = [784, 500, latent_dim, 500, 784]\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# parameter initializations \n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Use Glorot and Bengio (2010) initialization for weights\n",
    "# initialize weights for input to hidden layer\n",
    "# size : (dims[0],dims[1])\n",
    "W1 = theano.shared(rng_np.uniform(-np.sqrt(6.0/(dims[0]+dims[1])), np.sqrt(6.0/(dims[0]+dims[1])), \n",
    "                                  size=dims[0:2]), borrow=True, name=\"W1\")                   \n",
    "\n",
    "# initialize weights for hidden to latent layer (mean)\n",
    "# size : (dims[1],dims[2])\n",
    "W2_mu = theano.shared(rng_np.uniform(-np.sqrt(6.0/(dims[1]+dims[2])), np.sqrt(6.0/(dims[1]+dims[2])), \n",
    "                                  size=dims[1:3]), borrow=True, name=\"W2_mu\")                   \n",
    "\n",
    "# initialize weights for hidden to latent layer (log variance)\n",
    "# size : (dims[1],dims[2])\n",
    "# note that in order to make variance parameter unconstrained\n",
    "# we choose to model log(variacne) instead of variance\n",
    "W2_logvar = theano.shared(rng_np.uniform(-np.sqrt(6.0/(dims[1]+dims[2])), np.sqrt(6.0/(dims[1]+dims[2])), \n",
    "                                  size=dims[1:3]), borrow=True, name=\"W2_logvar\") \n",
    "\n",
    "# initialize weights for latent layer (z) to second hidden layer \n",
    "# size : (dims[2],dims[3])\n",
    "W3 = theano.shared(rng_np.uniform(-np.sqrt(6.0/(dims[2]+dims[3])), np.sqrt(6.0/(dims[2]+dims[3])), \n",
    "                                  size=dims[2:4]), borrow=True, name=\"W3\") \n",
    "\n",
    "# initilize weights from second hidden layer to output layer f(z)\n",
    "# size : (dims[3],dims[4])\n",
    "W4 = theano.shared(rng_np.uniform(-np.sqrt(6.0/(dims[3]+dims[4])), np.sqrt(6.0/(dims[3]+dims[4])), \n",
    "                                  size=dims[3:5]), borrow=True, name=\"W4\") \n",
    "\n",
    "# initialize bias for first hidden layer\n",
    "# size : (dims[1],)\n",
    "b1 = theano.shared(np.zeros((dims[1],)), borrow=True, name=\"b1\")\n",
    "\n",
    "# initialize bias for latent layer (mean)\n",
    "# size : (dims[2],)\n",
    "b2_mu = theano.shared(np.zeros((dims[2],)), borrow=True, name=\"b2_mu\")\n",
    "\n",
    "# initialize bias for latent layer (log variance)\n",
    "# size : (dims[2],)\n",
    "b2_logvar = theano.shared(np.zeros((dims[2],)), borrow=True, name=\"b2_logvar\")\n",
    "\n",
    "# initialize bias for the second hidden layer\n",
    "# size : (dims[3],)\n",
    "b3 = theano.shared(np.zeros((dims[3],)), borrow=True, name=\"b3\")\n",
    "\n",
    "# initialize bias for the output layer \n",
    "# size : (dims[4],) \n",
    "b4 = theano.shared(np.zeros((dims[4],)), borrow=True, name=\"b4\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# define feed-forward computations\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# compute non-linear activations in the first hidden layer\n",
    "# hint : use input, W1, and b1\n",
    "x1     = T.tanh(T.dot(input, W1) + b1)\n",
    "\n",
    "# compute linear activation for latent mean\n",
    "# hint : use x1, W2_mu, b2_mu\n",
    "mu     = T.dot(x1, W2_mu) + b2_mu \n",
    "\n",
    "# compute linear activation for latent logvar\n",
    "# hint : use x1, W2_logvar, b2_logvar\n",
    "logvar = T.dot(x1, W2_logvar) + b2_logvar\n",
    "\n",
    "# compute z\n",
    "# z = mu + {exp(0.5*logvar) * rnd_sample}\n",
    "# where rnd_sample is from standard normal\n",
    "# hint : pick samples of size (num_observation,dims[2])\n",
    "# hint : use srng for reproducibility \n",
    "z      = mu + (T.exp(0.5*logvar) * srng.normal((mb_size,dims[2])))\n",
    "\n",
    "# compute non-linear activation at second hidden layer\n",
    "# hint: use z, W3, b3\n",
    "x1_rec = T.tanh(T.dot(z, W3) + b3)\n",
    "\n",
    "# compute networks ouputs f(z)\n",
    "# hint : add non-linearity to scale output btw 0-1\n",
    "# hint : use x_rec, W4, b4\n",
    "x_rec  = T.nnet.sigmoid(T.dot(x1_rec, W4) + b4)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# back propagation computations\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# compute lower_bound = reconstruction cost - KL cost\n",
    "# minimie negative of lower bound i.e. cost = -lower_bound\n",
    "\n",
    "# hint reconstruction cost for each observation \n",
    "# can be computed as sum of binary cross entropy \n",
    "# across all dimensions \n",
    "cost_rec = T.sum(-T.nnet.binary_crossentropy(x_rec, input)) / mb_size\n",
    "\n",
    "# hint KL cost for each observation \n",
    "# can be computed using the formula given above\n",
    "cost_KL  = (-0.5 / mb_size) * (mu.shape[0]*mu.shape[1] + T.sum(2.* logvar) -\n",
    "                              T.sum(T.square(mu)) - T.sum(T.exp(2. * logvar))) / mb_size\n",
    "# compute lower bound for each observation\n",
    "# as reconstruction - kl cost\n",
    "lower_bound = cost_rec - cost_KL\n",
    "\n",
    "# final cost function scalar to minimize\n",
    "# can be computed as negative mean of lower bound\n",
    "cost = -T.mean(lower_bound)\n",
    "\n",
    "\n",
    "\n",
    "# create a list of all the parameters to be optimized\n",
    "params = [W1,W2_mu,W2_logvar,W3,W4,b1,b2_mu,b2_logvar,b3,b4]\n",
    "\n",
    "\n",
    "# pass cost and params list to \n",
    "# gradient_updates_Adam for optimization\n",
    "# above function returns updates list for training\n",
    "# refer to assignment3 for more details\n",
    "updates = gradient_updates_Adam(cost,params,learning_rate,eps = 1e-8,beta1 = 0.9,beta2 = 0.99)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# define theano functions\n",
    "# ------------------------------------------------------\n",
    "\n",
    "\n",
    "# define vae_train function \n",
    "# inputs : minibatch-index and learning rate\n",
    "# ouputs : cost, cost_rec and cost_KL\n",
    "# updates: updates from Adam\n",
    "# givens : symobolic input as subset of \n",
    "#        : train_x over minibatch index\n",
    "vae_train = theano.function(\n",
    "        inputs = [mbidxs,learning_rate],\n",
    "        outputs = [cost, cost_rec, cost_KL], \n",
    "        updates = updates,\n",
    "        givens = {input : train_x[mbidxs,:],}\n",
    "    )\n",
    "\n",
    "# define vae_cost function\n",
    "# inputs : input observation matrix\n",
    "# outputs: cost, cost_rec and cost_KL\n",
    "vae_cost = theano.function(\n",
    "        inputs = [input],\n",
    "        outputs = [cost, cost_rec, cost_KL]\n",
    "    )\n",
    "\n",
    "# define vae_predict function\n",
    "# inputs : input observation matrix\n",
    "# outputs: f(z), latent mu and logvar\n",
    "vae_predict = theano.function(\n",
    "        inputs = [input],\n",
    "        outputs = [x_rec, mu, logvar]\n",
    "    )\n",
    "\n",
    "\n",
    "# define vae_generate function\n",
    "# inputs : input z\n",
    "# outputs: f(z) or output of the final layer\n",
    "vae_generate = theano.function(\n",
    "        inputs = [z],\n",
    "        outputs = [x_rec]\n",
    "    )\n",
    "\n",
    "# define vae_z function\n",
    "# inputs : input observations\n",
    "# outputs: z or latent space for the input\n",
    "vae_z = theano.function(\n",
    "        inputs = [input],\n",
    "        outputs = [z]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "###################################################################\n",
    "############################ END ##################################\n",
    "###################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** training started  *****\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input dimension mis-match. (input[0].shape[0] = 1666, input[3].shape[0] = 50)\nApply node that caused the error: Elemwise{Composite{(i0 + (exp((i1 * i2)) * i3))}}(Elemwise{Add}[(0, 0)].0, TensorConstant{(1, 1) of 0.5}, Elemwise{Add}[(0, 0)].0, Reshape{2}.0)\nToposort index: 23\nInputs types: [TensorType(float64, matrix), TensorType(float64, (True, True)), TensorType(float64, matrix), TensorType(float32, matrix)]\nInputs shapes: [(1666, 2), (1, 1), (1666, 2), (50, 2)]\nInputs strides: [(16, 8), (8, 8), (16, 8), (8, 4)]\nInputs values: ['not shown', array([[ 0.5]]), 'not shown', 'not shown']\nOutputs clients: [[Dot22(Elemwise{Composite{(i0 + (exp((i1 * i2)) * i3))}}.0, W3)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input dimension mis-match. (input[0].shape[0] = 1666, input[3].shape[0] = 50)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-67200f3c02c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mvalid_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mtrain_costs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_minib_costs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    896\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    899\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input dimension mis-match. (input[0].shape[0] = 1666, input[3].shape[0] = 50)\nApply node that caused the error: Elemwise{Composite{(i0 + (exp((i1 * i2)) * i3))}}(Elemwise{Add}[(0, 0)].0, TensorConstant{(1, 1) of 0.5}, Elemwise{Add}[(0, 0)].0, Reshape{2}.0)\nToposort index: 23\nInputs types: [TensorType(float64, matrix), TensorType(float64, (True, True)), TensorType(float64, matrix), TensorType(float32, matrix)]\nInputs shapes: [(1666, 2), (1, 1), (1666, 2), (50, 2)]\nInputs strides: [(16, 8), (8, 8), (16, 8), (8, 4)]\nInputs values: ['not shown', array([[ 0.5]]), 'not shown', 'not shown']\nOutputs clients: [[Dot22(Elemwise{Composite{(i0 + (exp((i1 * i2)) * i3))}}.0, W3)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "# define training parameters\n",
    "mb_size = 50\n",
    "epochs = 20\n",
    "lrate = 0.001\n",
    "train_costs = []\n",
    "valid_costs = []\n",
    "train_start_time = time.time()\n",
    "\n",
    "train_size    = len(train_y.eval()) \n",
    "training_idxs = np.arange(train_size)\n",
    "\n",
    "print(\"***** training started  *****\")\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    rng_np.shuffle(training_idxs)\n",
    "                \n",
    "    try:    \n",
    "        # initialize empty lists to collect costs\n",
    "        t_minib_costs = []\n",
    "        t_minib_cost_kl = []\n",
    "        t_minib_cost_rec = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_startidx in range(0, train_size, mb_size):\n",
    "\n",
    "            mini_batch = training_idxs[batch_startidx:batch_startidx+mb_size]\n",
    "            \n",
    "            # MODEL TRAINING CALL\n",
    "            mb_train_cost, mb_cost_rec,mb_cost_kl  = vae_train(\n",
    "                    mbidxs = np.asarray(mini_batch).astype(np.int32),\n",
    "                    learning_rate=lrate)\n",
    "            \n",
    "            # collect costs\n",
    "            t_minib_costs.append(np.mean(mb_train_cost))\n",
    "            t_minib_cost_kl.append(np.mean(mb_cost_kl))\n",
    "            t_minib_cost_rec.append(np.mean(mb_cost_rec))\n",
    "            \n",
    "            \n",
    "        valid_cost, _, _ = vae_cost(input = valid_x.get_value())\n",
    "        \n",
    "        train_costs.append(np.mean(t_minib_costs))\n",
    "        valid_costs.append(np.mean(valid_cost))\n",
    "        \n",
    "        print(\"epoch={:<3d} -- train_cost={:>6.4f} -- valid_cost={:>6.4f} -- cost(Rec,KL)=({:>6.4f},{:>6.4f}) -- time={:>5.3f}\".format(epoch+1, train_costs[-1], valid_costs[-1], np.mean(t_minib_cost_rec),np.mean(t_minib_cost_kl),time.time()-start_time))    \n",
    "    \n",
    "    except KeyboardInterrupt as e:\n",
    "        print(\"***** stopping *****\")\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot training & validation cost across iterations\n",
    "plt.plot(range(len(train_costs)), train_costs, \"r\", label=\"train\")\n",
    "plt.plot(range(len(valid_costs)), valid_costs, \"g\", label=\"valid\")\n",
    "plt.title(\"cost across iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, the training and validation cost across iterations should have started going down. However, model might not be trained enough. In the following blocks, latent space and reconstructions are visualized. Don't panic if all reconstructions look like blurry eggs. Model has not been trained enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent space can be visualized as shown in the plot below. Location of each ellipse is determined by the mean value $\\mu(\\mathbf{x})$ and width is determined by value of the standard deviation corresponding to the variance $\\Sigma\\mathbf{(x)}$ for each observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec, latent_mu,latent_logvar = vae_predict(train_x.get_value())\n",
    "plot_latent_space2D(latent_mu,latent_logvar,train_y.eval(),\"training set : latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual = train_x.get_value()[:20,:]\n",
    "recons,_,_ = vae_predict(actual)\n",
    "plot_act_rec(actual,recons,\"training set : mean of sampling distribution p(X) for a randomly sampled z from the latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec, latent_mu, latent_logvar = vae_predict(valid_x.get_value())\n",
    "plot_latent_space2D(latent_mu,latent_logvar,valid_y.eval(),\"validation set : latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual = valid_x.get_value()[:20,:]\n",
    "recons,_,_ = vae_predict(actual)\n",
    "plot_act_rec(actual,recons,\"validation set : mean of sampling distribution p(X) for a randomly sampled z from the latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec, latent_mu,latent_logvar = vae_predict(test_x.get_value())\n",
    "plot_latent_space2D(latent_mu,latent_logvar,test_y.eval(),\"test set : latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual = test_x.get_value()[:20,:]\n",
    "recons,_,_ = vae_predict(actual)\n",
    "plot_act_rec(actual,recons,\"test set : mean of sampling distribution p(X) for a randomly sampled z from the latent space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate mean of the data sampling distribution $p(\\mathbf{x})$ by drawing random latent samples from the prior $p(\\mathbf{z})$ and then passing it through the decoder. \n",
    "\n",
    "Note that these are not yet data samples, but rather means of $p(\\mathbf{x})$ for each observation. In order to generate true data samples, one will have to take Bernoulli samples from this sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_data = np.random.normal(size=(20,2)).astype(np.float32)\n",
    "samples = vae_generate(rand_data)\n",
    "\n",
    "plot_img(samples,\n",
    "            name=\"randomly chosen means of sampling distribution from the fully trained model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate samples from the visualize them on a 2D grid. *plot_sample_2d* take samples uniformly on a 2D grid for the latent space $\\mathbf{z}$, and generates network constructions for these samples.\n",
    "\n",
    "\n",
    "Note that *sample_latent* is used only to *check min & max range for latent dimensions* within which the grid samples are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_sample_2d(sample_latent,name=None):\n",
    "    matplotlib.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "    min_z = np.min(sample_latent,axis=0)\n",
    "    max_z = np.max(sample_latent,axis=0)\n",
    "\n",
    "    idx = np.arange(min_z.shape[0]);\n",
    "    np.random.shuffle(idx)\n",
    "    ld_idx = idx[:2]\n",
    "    minz = min_z[ld_idx]\n",
    "    maxz = max_z[ld_idx]\n",
    "\n",
    "    img = np.zeros((28*20, 28*20))\n",
    "    zvals_1 = np.linspace(minz[0], maxz[0], 20)\n",
    "    zvals_2 = np.linspace(minz[1], maxz[1], 20)\n",
    "    for i, z1 in enumerate(zvals_1):\n",
    "        for j, z2 in enumerate(zvals_2):\n",
    "            z_sample = np.zeros(sample_latent[0].shape)\n",
    "            np.put(z_sample,ld_idx,[z1,z2])\n",
    "            xp = vae_generate([z_sample])\n",
    "            img[(i*28):((i+1)*28), (j*28):((j+1)*28)] = xp.reshape((28, 28))\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "    im = ax.imshow(img,cmap='gray')\n",
    "    ax.xaxis.set_ticklabels([])\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "\n",
    "    if name is not None:\n",
    "        ax.set_title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_latent = vae_z(test_x.get_value())\n",
    "plot_sample_2d(sample_latent,name=\"means of the sampling distribution across 2D grid of the latent space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pre-trained model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, Theano shared variables for all the weights and biases are initialized with pre-trained weights. Above model was trained with all 50k training samples for 500 epochs with 500 mini-batch size. Adam optimizer was used with learning rate = 0.0001 (and default value for other Adam parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading pre-trained weights\n",
    "f = open('pretrained/vae_2ld.save', 'rb')\n",
    "pretrained_weights = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained weights and biases\n",
    "W1.set_value(pretrained_weights['W1'])\n",
    "W2_mu.set_value(pretrained_weights['W2_mu'])\n",
    "W2_logvar.set_value(pretrained_weights['W2_logvar'])\n",
    "W3.set_value(pretrained_weights['W3'])\n",
    "W4.set_value(pretrained_weights['W4'])\n",
    "b1.set_value(pretrained_weights['b1'])\n",
    "b2_mu.set_value(pretrained_weights['b2_mu'])\n",
    "b2_logvar.set_value(pretrained_weights['b2_logvar'])\n",
    "b3.set_value(pretrained_weights['b3'])\n",
    "b4.set_value(pretrained_weights['b4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec, latent_mu,latent_logvar = vae_predict(train_x.get_value())\n",
    "plot_latent_space2D(latent_mu,latent_logvar,train_y.eval(),\"training set : latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual = train_x.get_value()[:20,:]\n",
    "recons,_,_ = vae_predict(actual)\n",
    "plot_act_rec(actual,recons,\"training set : mean of sampling distribution p(X) for a randomly sampled z from the latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec, latent_mu, latent_logvar = vae_predict(valid_x.get_value())\n",
    "plot_latent_space2D(latent_mu,latent_logvar,valid_y.eval(),\"validation set : latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual = valid_x.get_value()[:20,:]\n",
    "recons,_,_ = vae_predict(actual)\n",
    "plot_act_rec(actual,recons,\"validation set : mean of sampling distribution p(X) for a randomly sampled z from the latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec, latent_mu,latent_logvar = vae_predict(test_x.get_value())\n",
    "plot_latent_space2D(latent_mu,latent_logvar,test_y.eval(),\"test set : latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual = test_x.get_value()[:20,:]\n",
    "recons,_,_ = vae_predict(actual)\n",
    "plot_act_rec(actual,recons,\"test set : mean of sampling distribution p(X) for a randomly sampled z from the latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_data = np.random.normal(size=(20,2)).astype(np.float32)\n",
    "samples = vae_generate(rand_data)\n",
    "\n",
    "plot_img(samples,\n",
    "            name=\"randomly chosen means of sampling distribution from the fully trained model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_latent = vae_z(test_x.get_value())\n",
    "plot_sample_2d(sample_latent,name=\"means of the sampling distribution across 2D grid of the latent space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Discussion (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the actual vs reconstructed plots which digits you think the models learned to reconstruct well?\n",
    "\n",
    "Are there any interesting observations on the latent space? Can reconstruction quality be understood by looking at posterior mean and variance? \n",
    "How can the sampling distribution plot on the 2D grid be connected to the above discussion?\n",
    "\n",
    "Add your comments in a cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately I'm suck with a dimension mismatch error, couldn't debug it after hours of effort. Using spyder I can see the minibatch cost is a list of 100 elements from 544 to around 200, so the code executed fine, but I don't know why it gave that error after 100 iterations. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
