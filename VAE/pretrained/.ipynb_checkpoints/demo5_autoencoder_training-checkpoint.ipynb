{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction and Generative Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import theano\n",
    "import time \n",
    "theano.config.floatX = \"float32\"\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.data import load_mnist_data\n",
    "from utils.pca import pca\n",
    "from exercise_helper import load_data,plot_2d_proj,plot_act_rec\n",
    "import six.moves.cPickle as pickle\n",
    "\n",
    "rng_np = np.random.RandomState(23455)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an auto-encoder we try to learn a neural network which has the following structure:\n",
    "\n",
    "`x                                  x*\n",
    "x                                  x*\n",
    "x       x1               x1*       x*\n",
    "x       x1       l       x1*       x*\n",
    "x ----> x1 ----> l ----> x1* ----> x*\n",
    "x       x1               x1*       x*\n",
    "x                                  x*\n",
    "x                                  x*`\n",
    "\n",
    "Assuming the column of $\\mathbf{x}$ represents our input, we project that into an intermediate vector (vector of $\\mathbf{x}1$) followed by the two-dimensional embedding vector. One way to think about this is data compression. \n",
    "\n",
    "If we try to compress the higher dimensional $\\mathbf{x}$ vector into two dimensions, we should be able to de-compress the two-dimensional embedding back into the original data. This is the motivation behind trying to **reconstruct** the data, where $\\mathbf{x*}$ is the reconstructed vector.\n",
    "\n",
    "Each projection, $\\mathbf{x} \\rightarrow \\mathbf{x1} \\rightarrow \\mathbf{l} \\rightarrow \\mathbf{x1*} \\rightarrow \\mathbf{x*}$ is done using a non-linear transformation i.e. $\\mathbf{x1} = \\sigma(W_{x\\_x1} \\times \\mathbf{x} + \\mathbf{b})$ where $W_{x\\_x1}$ and $\\mathbf{b}$ are weights and biases which are learnt from the training dataset using some gradient based optimization method (stochastic gradient descent) by optimizing (minimizing) $\\texttt{ mean}(\\mathbf{x} - \\mathbf{x*})^{2}$ which is the squared difference between the original vector and reconstructed vector  , as the cost function.\n",
    "\n",
    "After training the auto-encoder we see how well it performs by projecting the test-set into a lower (two) dimension space and plotting the result. \n",
    "Note: Weight initializations are done using equation 16 (Glorot and Bengio 2010) according to which $W \\sim \\mathbf{U}\\left( -\\sqrt{\\frac{6}{n_\\text{in} + n_\\text{out}}},  \\sqrt{\\frac{6}{n_\\text{in} + n_\\text{out}}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Loading data *****\n"
     ]
    }
   ],
   "source": [
    "datasets = load_data('mnist.pkl.gz', train_size = 50000,binarize = \"stochastic\")\n",
    "train_x, train_y = datasets[0]\n",
    "valid_x, valid_y = datasets[1]\n",
    "test_x, test_y = datasets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_updates_Adam(cost, params, learning_rate,eps = 1e-8,beta1 = 0.9,beta2 = 0.999):\n",
    "    # Function to return an update list for the parameters to be updated\n",
    "    \n",
    "    # cost: MSE cost Theano variable\n",
    "    # params :  parameters coming from hidden and output layers\n",
    "    # learning rate: learning rate defined as hyperparameter\n",
    "    \n",
    "    # Outputs:\n",
    "    # updates : updates to be made and to be defined in the train_model function. \n",
    "    updates = []\n",
    "    for param in params:\n",
    "             \n",
    "            t = theano.shared(1)\n",
    "            s = theano.shared(param.get_value(borrow=True)*0.)\n",
    "            r = theano.shared(param.get_value(borrow=True)*0.)\n",
    "            updates.append((s, beta1*s + (1.0-beta1)*T.grad(cost, param) ))\n",
    "            updates.append((r, beta2*r + (1.0-beta2)*(T.grad(cost, param)**2) ))\n",
    "            s_hat =  s/(1-beta1**t)\n",
    "            r_hat =  r/(1-beta2**t)\n",
    "            updates.append((param, param - learning_rate*s_hat/(np.sqrt(r_hat)+eps) ))\n",
    "            updates.append((t, t+1)) \n",
    "\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implementing the autoencoder in theano\n",
    "# ensure similar network architecture to (Kingma and Welling 2013)\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "input = T.fmatrix(\"input\") # symbolic float32 matrix, eventually will have (minibatch x 784)\n",
    "\n",
    "learning_rate = T.fscalar(\"learning_rate\") # float32 learning rate.\n",
    "mbidxs = T.lvector(\"mbidxs\")\n",
    "# autoencoder layer dimensions\n",
    "dims = [784, 500, 2, 500, 784]\n",
    "\n",
    "uniform_lim = lambda n_in,n_out : (-np.sqrt(6.0/(n_in+n_out)), np.sqrt(6.0/(n_in+n_out))) \n",
    "\n",
    "# Glorot and Bengio initialization\n",
    "W1 = theano.shared(rng_np.uniform(*uniform_lim(dims[0], dims[1]), size=dims[0:2]), borrow=True, name=\"W1\") # n_in = 784, n_out = 300\n",
    "W2 = theano.shared(rng_np.uniform(*uniform_lim(dims[1], dims[2]), size=dims[1:3]), borrow=True, name=\"W2\") # n_in = 300, n_out = 2\n",
    "W3 = theano.shared(rng_np.uniform(*uniform_lim(dims[2], dims[3]), size=dims[2:4]), borrow=True, name=\"W3\") # n_in = 2, n_out = 300\n",
    "W4 = theano.shared(rng_np.uniform(*uniform_lim(dims[3], dims[4]), size=dims[3:5]), borrow=True, name=\"W4\") # n_in = 300, n_out = 784\n",
    "\n",
    "# biases are all initialized to zeros\n",
    "b1 = theano.shared(np.zeros((dims[1],)), borrow=True, name=\"b1\")\n",
    "b2 = theano.shared(np.zeros((dims[2],)), borrow=True, name=\"b2\")\n",
    "b3 = theano.shared(np.zeros((dims[3],)), borrow=True, name=\"b3\")\n",
    "b4 = theano.shared(np.zeros((dims[4],)), borrow=True, name=\"b4\")\n",
    "\n",
    "params = [W1,W2,W3,W4,b1,b2,b3,b4]\n",
    "\n",
    "srng = RandomStreams(seed=1234)\n",
    "sigmoid = T.nnet.sigmoid\n",
    "tanh = T.tanh\n",
    "\n",
    "# randomly mask input\n",
    "_input  = srng.binomial(n=1, p=0.95, size=input.shape) # 0.3 = (1-0.7) is the input masking/corruption rate\n",
    "x1     = tanh(T.dot(input*_input, W1) + b1)\n",
    "l      = T.dot(x1, W2) + b2\n",
    "x1_rec = tanh(T.dot(l, W3) + b3)\n",
    "x_rec  = sigmoid(T.dot(x1_rec, W4) + b4)\n",
    "\n",
    "# cross entropy cost\n",
    "cost = -T.mean(input * T.log(x_rec) + (1-input) * T.log(1-x_rec)) \n",
    "\n",
    "updates = gradient_updates_Adam(cost,params,learning_rate)\n",
    "\n",
    "autoencoder_train = theano.function(\n",
    "        inputs = [mbidxs, learning_rate], \n",
    "        outputs = [cost, x_rec, l],\n",
    "        updates = updates,\n",
    "        givens = {\n",
    "            input : train_x[mbidxs,:],\n",
    "        }) # takes an input and returns its reconstruction_error, reconstruction and lower dimensional representation.\n",
    "\n",
    "autoencoder_predict = theano.function(\n",
    "        inputs = [input],\n",
    "        outputs = [cost, x_rec, l]\n",
    "    ) # just returns reconstructions and lower dimensional representation, no training happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** training started  *****\n",
      "epoch =  0 -- train_error = 0.3233 -- valid_error = 0.2469 -- time = 12.631\n",
      "epoch =  1 -- train_error = 0.2443 -- valid_error = 0.2389 -- time = 9.085\n",
      "epoch =  2 -- train_error = 0.2406 -- valid_error = 0.2370 -- time = 9.345\n",
      "epoch =  3 -- train_error = 0.2361 -- valid_error = 0.2336 -- time = 9.691\n",
      "epoch =  4 -- train_error = 0.2327 -- valid_error = 0.2308 -- time = 9.143\n",
      "epoch =  5 -- train_error = 0.2307 -- valid_error = 0.2262 -- time = 10.655\n",
      "epoch =  6 -- train_error = 0.2301 -- valid_error = 0.2296 -- time = 12.811\n",
      "epoch =  7 -- train_error = 0.2260 -- valid_error = 0.2233 -- time = 13.278\n",
      "epoch =  8 -- train_error = 0.2229 -- valid_error = 0.2193 -- time = 13.402\n",
      "epoch =  9 -- train_error = 0.2211 -- valid_error = 0.2185 -- time = 13.858\n",
      "epoch = 10 -- train_error = 0.2193 -- valid_error = 0.2184 -- time = 13.025\n",
      "epoch = 11 -- train_error = 0.2180 -- valid_error = 0.2172 -- time = 17.975\n",
      "epoch = 12 -- train_error = 0.2170 -- valid_error = 0.2155 -- time = 20.112\n",
      "epoch = 13 -- train_error = 0.2161 -- valid_error = 0.2160 -- time = 20.261\n",
      "epoch = 14 -- train_error = 0.2151 -- valid_error = 0.2141 -- time = 18.468\n",
      "epoch = 15 -- train_error = 0.2144 -- valid_error = 0.2130 -- time = 19.829\n",
      "epoch = 16 -- train_error = 0.2134 -- valid_error = 0.2102 -- time = 18.554\n",
      "epoch = 17 -- train_error = 0.2125 -- valid_error = 0.2095 -- time = 15.993\n",
      "epoch = 18 -- train_error = 0.2118 -- valid_error = 0.2118 -- time = 20.129\n",
      "epoch = 19 -- train_error = 0.2108 -- valid_error = 0.2093 -- time = 19.005\n",
      "epoch = 20 -- train_error = 0.2098 -- valid_error = 0.2080 -- time = 18.772\n",
      "epoch = 21 -- train_error = 0.2093 -- valid_error = 0.2080 -- time = 20.650\n",
      "epoch = 22 -- train_error = 0.2090 -- valid_error = 0.2069 -- time = 18.838\n",
      "epoch = 23 -- train_error = 0.2083 -- valid_error = 0.2078 -- time = 19.605\n",
      "epoch = 24 -- train_error = 0.2073 -- valid_error = 0.2062 -- time = 18.516\n",
      "epoch = 25 -- train_error = 0.2067 -- valid_error = 0.2059 -- time = 18.007\n",
      "epoch = 26 -- train_error = 0.2063 -- valid_error = 0.2055 -- time = 19.398\n",
      "epoch = 27 -- train_error = 0.2059 -- valid_error = 0.2054 -- time = 19.447\n",
      "epoch = 28 -- train_error = 0.2054 -- valid_error = 0.2025 -- time = 19.963\n",
      "epoch = 29 -- train_error = 0.2048 -- valid_error = 0.2016 -- time = 18.926\n",
      "epoch = 30 -- train_error = 0.2035 -- valid_error = 0.2030 -- time = 19.565\n",
      "epoch = 31 -- train_error = 0.2028 -- valid_error = 0.2016 -- time = 17.683\n",
      "epoch = 32 -- train_error = 0.2022 -- valid_error = 0.1997 -- time = 19.652\n",
      "epoch = 33 -- train_error = 0.2023 -- valid_error = 0.2009 -- time = 19.482\n",
      "epoch = 34 -- train_error = 0.2028 -- valid_error = 0.1994 -- time = 18.850\n",
      "epoch = 35 -- train_error = 0.2019 -- valid_error = 0.2002 -- time = 18.976\n",
      "epoch = 36 -- train_error = 0.2010 -- valid_error = 0.1991 -- time = 17.928\n",
      "epoch = 37 -- train_error = 0.2003 -- valid_error = 0.1996 -- time = 20.144\n",
      "epoch = 38 -- train_error = 0.2001 -- valid_error = 0.1975 -- time = 18.646\n",
      "epoch = 39 -- train_error = 0.2001 -- valid_error = 0.1994 -- time = 19.387\n",
      "epoch = 40 -- train_error = 0.2002 -- valid_error = 0.1986 -- time = 21.758\n",
      "epoch = 41 -- train_error = 0.1995 -- valid_error = 0.1974 -- time = 20.471\n",
      "epoch = 42 -- train_error = 0.1984 -- valid_error = 0.1966 -- time = 18.692\n",
      "epoch = 43 -- train_error = 0.1979 -- valid_error = 0.1962 -- time = 17.519\n",
      "epoch = 44 -- train_error = 0.1975 -- valid_error = 0.1964 -- time = 19.814\n",
      "epoch = 45 -- train_error = 0.1978 -- valid_error = 0.1976 -- time = 18.056\n",
      "epoch = 46 -- train_error = 0.1980 -- valid_error = 0.1973 -- time = 19.779\n",
      "epoch = 47 -- train_error = 0.1974 -- valid_error = 0.1963 -- time = 17.569\n",
      "epoch = 48 -- train_error = 0.1966 -- valid_error = 0.1953 -- time = 19.715\n",
      "epoch = 49 -- train_error = 0.1968 -- valid_error = 0.1947 -- time = 19.432\n",
      "epoch = 50 -- train_error = 0.1964 -- valid_error = 0.1942 -- time = 17.794\n",
      "epoch = 51 -- train_error = 0.1960 -- valid_error = 0.1939 -- time = 17.809\n",
      "epoch = 52 -- train_error = 0.1956 -- valid_error = 0.1941 -- time = 17.387\n",
      "epoch = 53 -- train_error = 0.1955 -- valid_error = 0.1944 -- time = 20.078\n",
      "epoch = 54 -- train_error = 0.1951 -- valid_error = 0.1935 -- time = 18.617\n",
      "epoch = 55 -- train_error = 0.1945 -- valid_error = 0.1941 -- time = 20.646\n",
      "epoch = 56 -- train_error = 0.1943 -- valid_error = 0.1942 -- time = 19.462\n",
      "epoch = 57 -- train_error = 0.1943 -- valid_error = 0.1936 -- time = 18.538\n",
      "epoch = 58 -- train_error = 0.1939 -- valid_error = 0.1927 -- time = 16.855\n",
      "epoch = 59 -- train_error = 0.1932 -- valid_error = 0.1919 -- time = 16.988\n",
      "epoch = 60 -- train_error = 0.1929 -- valid_error = 0.1915 -- time = 18.032\n",
      "epoch = 61 -- train_error = 0.1924 -- valid_error = 0.1908 -- time = 18.610\n",
      "epoch = 62 -- train_error = 0.1925 -- valid_error = 0.1908 -- time = 18.805\n",
      "epoch = 63 -- train_error = 0.1926 -- valid_error = 0.1913 -- time = 18.815\n",
      "epoch = 64 -- train_error = 0.1928 -- valid_error = 0.1919 -- time = 18.912\n",
      "epoch = 65 -- train_error = 0.1929 -- valid_error = 0.1925 -- time = 19.320\n",
      "epoch = 66 -- train_error = 0.1934 -- valid_error = 0.1912 -- time = 19.323\n",
      "epoch = 67 -- train_error = 0.1929 -- valid_error = 0.1919 -- time = 18.397\n",
      "epoch = 68 -- train_error = 0.1918 -- valid_error = 0.1898 -- time = 19.814\n",
      "epoch = 69 -- train_error = 0.1908 -- valid_error = 0.1900 -- time = 17.905\n",
      "epoch = 70 -- train_error = 0.1905 -- valid_error = 0.1890 -- time = 20.740\n",
      "epoch = 71 -- train_error = 0.1901 -- valid_error = 0.1889 -- time = 20.134\n",
      "epoch = 72 -- train_error = 0.1897 -- valid_error = 0.1897 -- time = 19.102\n",
      "epoch = 73 -- train_error = 0.1899 -- valid_error = 0.1899 -- time = 17.896\n",
      "epoch = 74 -- train_error = 0.1906 -- valid_error = 0.1918 -- time = 17.149\n",
      "epoch = 75 -- train_error = 0.1914 -- valid_error = 0.1921 -- time = 19.653\n",
      "epoch = 76 -- train_error = 0.1909 -- valid_error = 0.1918 -- time = 18.934\n",
      "epoch = 77 -- train_error = 0.1904 -- valid_error = 0.1897 -- time = 19.400\n",
      "epoch = 78 -- train_error = 0.1898 -- valid_error = 0.1892 -- time = 18.328\n",
      "epoch = 79 -- train_error = 0.1898 -- valid_error = 0.1878 -- time = 18.180\n",
      "epoch = 80 -- train_error = 0.1892 -- valid_error = 0.1876 -- time = 18.488\n",
      "epoch = 81 -- train_error = 0.1887 -- valid_error = 0.1878 -- time = 17.999\n",
      "epoch = 82 -- train_error = 0.1884 -- valid_error = 0.1880 -- time = 19.704\n",
      "epoch = 83 -- train_error = 0.1887 -- valid_error = 0.1877 -- time = 18.327\n",
      "epoch = 84 -- train_error = 0.1895 -- valid_error = 0.1896 -- time = 17.817\n",
      "epoch = 85 -- train_error = 0.1893 -- valid_error = 0.1893 -- time = 18.765\n",
      "epoch = 86 -- train_error = 0.1885 -- valid_error = 0.1880 -- time = 18.447\n",
      "epoch = 87 -- train_error = 0.1881 -- valid_error = 0.1874 -- time = 18.541\n",
      "epoch = 88 -- train_error = 0.1873 -- valid_error = 0.1863 -- time = 19.117\n",
      "epoch = 89 -- train_error = 0.1880 -- valid_error = 0.1883 -- time = 19.621\n",
      "epoch = 90 -- train_error = 0.1886 -- valid_error = 0.1869 -- time = 19.266\n",
      "epoch = 91 -- train_error = 0.1879 -- valid_error = 0.1865 -- time = 18.311\n",
      "epoch = 92 -- train_error = 0.1871 -- valid_error = 0.1866 -- time = 16.980\n",
      "epoch = 93 -- train_error = 0.1872 -- valid_error = 0.1861 -- time = 19.100\n",
      "epoch = 94 -- train_error = 0.1876 -- valid_error = 0.1872 -- time = 18.781\n",
      "epoch = 95 -- train_error = 0.1871 -- valid_error = 0.1863 -- time = 17.920\n",
      "epoch = 96 -- train_error = 0.1874 -- valid_error = 0.1885 -- time = 19.692\n",
      "epoch = 97 -- train_error = 0.1874 -- valid_error = 0.1871 -- time = 18.218\n",
      "epoch = 98 -- train_error = 0.1869 -- valid_error = 0.1857 -- time = 18.801\n",
      "epoch = 99 -- train_error = 0.1869 -- valid_error = 0.1863 -- time = 18.910\n",
      "epoch =100 -- train_error = 0.1868 -- valid_error = 0.1873 -- time = 19.619\n",
      "epoch =101 -- train_error = 0.1861 -- valid_error = 0.1859 -- time = 18.785\n",
      "epoch =102 -- train_error = 0.1856 -- valid_error = 0.1861 -- time = 18.990\n",
      "epoch =103 -- train_error = 0.1858 -- valid_error = 0.1874 -- time = 20.180\n",
      "epoch =104 -- train_error = 0.1860 -- valid_error = 0.1865 -- time = 19.266\n",
      "epoch =105 -- train_error = 0.1861 -- valid_error = 0.1849 -- time = 19.571\n",
      "epoch =106 -- train_error = 0.1855 -- valid_error = 0.1866 -- time = 17.492\n",
      "epoch =107 -- train_error = 0.1857 -- valid_error = 0.1863 -- time = 19.868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =108 -- train_error = 0.1853 -- valid_error = 0.1848 -- time = 20.060\n",
      "epoch =109 -- train_error = 0.1852 -- valid_error = 0.1857 -- time = 17.762\n",
      "epoch =110 -- train_error = 0.1859 -- valid_error = 0.1879 -- time = 18.078\n",
      "epoch =111 -- train_error = 0.1860 -- valid_error = 0.1860 -- time = 19.037\n",
      "epoch =112 -- train_error = 0.1857 -- valid_error = 0.1846 -- time = 17.046\n",
      "epoch =113 -- train_error = 0.1858 -- valid_error = 0.1847 -- time = 19.350\n",
      "epoch =114 -- train_error = 0.1850 -- valid_error = 0.1855 -- time = 19.337\n",
      "epoch =115 -- train_error = 0.1847 -- valid_error = 0.1855 -- time = 19.022\n",
      "epoch =116 -- train_error = 0.1842 -- valid_error = 0.1849 -- time = 19.313\n",
      "epoch =117 -- train_error = 0.1842 -- valid_error = 0.1857 -- time = 18.056\n",
      "epoch =118 -- train_error = 0.1853 -- valid_error = 0.1883 -- time = 19.853\n",
      "epoch =119 -- train_error = 0.1854 -- valid_error = 0.1861 -- time = 17.942\n",
      "epoch =120 -- train_error = 0.1844 -- valid_error = 0.1855 -- time = 18.544\n",
      "epoch =121 -- train_error = 0.1840 -- valid_error = 0.1860 -- time = 17.765\n",
      "epoch =122 -- train_error = 0.1839 -- valid_error = 0.1858 -- time = 18.103\n",
      "epoch =123 -- train_error = 0.1843 -- valid_error = 0.1859 -- time = 19.868\n",
      "epoch =124 -- train_error = 0.1852 -- valid_error = 0.1866 -- time = 18.644\n",
      "epoch =125 -- train_error = 0.1845 -- valid_error = 0.1834 -- time = 17.294\n",
      "epoch =126 -- train_error = 0.1834 -- valid_error = 0.1837 -- time = 17.082\n",
      "epoch =127 -- train_error = 0.1833 -- valid_error = 0.1842 -- time = 19.064\n",
      "epoch =128 -- train_error = 0.1831 -- valid_error = 0.1841 -- time = 18.285\n",
      "epoch =129 -- train_error = 0.1830 -- valid_error = 0.1839 -- time = 19.160\n",
      "epoch =130 -- train_error = 0.1842 -- valid_error = 0.1880 -- time = 16.849\n",
      "epoch =131 -- train_error = 0.1848 -- valid_error = 0.1873 -- time = 17.743\n",
      "epoch =132 -- train_error = 0.1840 -- valid_error = 0.1848 -- time = 18.205\n",
      "epoch =133 -- train_error = 0.1833 -- valid_error = 0.1831 -- time = 18.668\n",
      "epoch =134 -- train_error = 0.1835 -- valid_error = 0.1850 -- time = 18.441\n",
      "epoch =135 -- train_error = 0.1837 -- valid_error = 0.1851 -- time = 19.456\n",
      "epoch =136 -- train_error = 0.1833 -- valid_error = 0.1830 -- time = 18.955\n",
      "epoch =137 -- train_error = 0.1826 -- valid_error = 0.1830 -- time = 20.225\n",
      "epoch =138 -- train_error = 0.1823 -- valid_error = 0.1833 -- time = 19.695\n",
      "epoch =139 -- train_error = 0.1831 -- valid_error = 0.1852 -- time = 17.555\n",
      "epoch =140 -- train_error = 0.1836 -- valid_error = 0.1857 -- time = 19.173\n",
      "epoch =141 -- train_error = 0.1827 -- valid_error = 0.1841 -- time = 18.554\n",
      "epoch =142 -- train_error = 0.1820 -- valid_error = 0.1835 -- time = 18.145\n",
      "epoch =143 -- train_error = 0.1822 -- valid_error = 0.1828 -- time = 17.138\n",
      "epoch =144 -- train_error = 0.1832 -- valid_error = 0.1860 -- time = 17.070\n",
      "epoch =145 -- train_error = 0.1834 -- valid_error = 0.1846 -- time = 18.803\n",
      "epoch =146 -- train_error = 0.1829 -- valid_error = 0.1844 -- time = 18.928\n",
      "epoch =147 -- train_error = 0.1824 -- valid_error = 0.1831 -- time = 18.405\n",
      "epoch =148 -- train_error = 0.1822 -- valid_error = 0.1825 -- time = 17.739\n",
      "epoch =149 -- train_error = 0.1822 -- valid_error = 0.1835 -- time = 18.916\n",
      "epoch =150 -- train_error = 0.1819 -- valid_error = 0.1835 -- time = 18.170\n",
      "epoch =151 -- train_error = 0.1820 -- valid_error = 0.1845 -- time = 18.608\n",
      "epoch =152 -- train_error = 0.1819 -- valid_error = 0.1840 -- time = 18.824\n",
      "epoch =153 -- train_error = 0.1818 -- valid_error = 0.1824 -- time = 19.690\n",
      "epoch =154 -- train_error = 0.1819 -- valid_error = 0.1824 -- time = 19.934\n",
      "epoch =155 -- train_error = 0.1816 -- valid_error = 0.1836 -- time = 19.761\n",
      "epoch =156 -- train_error = 0.1814 -- valid_error = 0.1834 -- time = 16.898\n",
      "epoch =157 -- train_error = 0.1816 -- valid_error = 0.1854 -- time = 18.140\n",
      "epoch =158 -- train_error = 0.1822 -- valid_error = 0.1831 -- time = 17.661\n",
      "epoch =159 -- train_error = 0.1822 -- valid_error = 0.1841 -- time = 18.717\n",
      "epoch =160 -- train_error = 0.1817 -- valid_error = 0.1831 -- time = 17.721\n",
      "epoch =161 -- train_error = 0.1813 -- valid_error = 0.1836 -- time = 18.252\n",
      "epoch =162 -- train_error = 0.1816 -- valid_error = 0.1840 -- time = 18.621\n",
      "epoch =163 -- train_error = 0.1815 -- valid_error = 0.1832 -- time = 19.804\n",
      "epoch =164 -- train_error = 0.1810 -- valid_error = 0.1823 -- time = 18.282\n",
      "epoch =165 -- train_error = 0.1806 -- valid_error = 0.1828 -- time = 18.933\n",
      "epoch =166 -- train_error = 0.1808 -- valid_error = 0.1837 -- time = 17.737\n",
      "epoch =167 -- train_error = 0.1811 -- valid_error = 0.1835 -- time = 17.167\n",
      "epoch =168 -- train_error = 0.1818 -- valid_error = 0.1824 -- time = 19.782\n",
      "epoch =169 -- train_error = 0.1822 -- valid_error = 0.1828 -- time = 17.737\n",
      "epoch =170 -- train_error = 0.1815 -- valid_error = 0.1821 -- time = 17.313\n",
      "epoch =171 -- train_error = 0.1808 -- valid_error = 0.1826 -- time = 18.287\n",
      "epoch =172 -- train_error = 0.1804 -- valid_error = 0.1824 -- time = 18.638\n",
      "epoch =173 -- train_error = 0.1803 -- valid_error = 0.1818 -- time = 17.513\n",
      "epoch =174 -- train_error = 0.1806 -- valid_error = 0.1829 -- time = 18.784\n",
      "epoch =175 -- train_error = 0.1814 -- valid_error = 0.1824 -- time = 17.590\n",
      "epoch =176 -- train_error = 0.1815 -- valid_error = 0.1820 -- time = 17.503\n",
      "epoch =177 -- train_error = 0.1808 -- valid_error = 0.1823 -- time = 18.619\n",
      "epoch =178 -- train_error = 0.1799 -- valid_error = 0.1821 -- time = 19.005\n",
      "epoch =179 -- train_error = 0.1805 -- valid_error = 0.1825 -- time = 18.711\n",
      "epoch =180 -- train_error = 0.1812 -- valid_error = 0.1829 -- time = 17.132\n",
      "epoch =181 -- train_error = 0.1803 -- valid_error = 0.1811 -- time = 18.480\n",
      "epoch =182 -- train_error = 0.1794 -- valid_error = 0.1816 -- time = 19.046\n",
      "epoch =183 -- train_error = 0.1794 -- valid_error = 0.1829 -- time = 17.906\n",
      "epoch =184 -- train_error = 0.1806 -- valid_error = 0.1814 -- time = 19.072\n",
      "epoch =185 -- train_error = 0.1808 -- valid_error = 0.1830 -- time = 19.192\n",
      "epoch =186 -- train_error = 0.1809 -- valid_error = 0.1851 -- time = 17.471\n",
      "epoch =187 -- train_error = 0.1801 -- valid_error = 0.1822 -- time = 18.600\n",
      "epoch =188 -- train_error = 0.1798 -- valid_error = 0.1818 -- time = 18.680\n",
      "epoch =189 -- train_error = 0.1801 -- valid_error = 0.1821 -- time = 16.900\n",
      "epoch =190 -- train_error = 0.1802 -- valid_error = 0.1826 -- time = 18.384\n",
      "epoch =191 -- train_error = 0.1802 -- valid_error = 0.1831 -- time = 18.402\n",
      "epoch =192 -- train_error = 0.1801 -- valid_error = 0.1835 -- time = 19.393\n",
      "epoch =193 -- train_error = 0.1797 -- valid_error = 0.1823 -- time = 20.308\n",
      "epoch =194 -- train_error = 0.1794 -- valid_error = 0.1810 -- time = 18.155\n",
      "epoch =195 -- train_error = 0.1797 -- valid_error = 0.1810 -- time = 19.098\n",
      "epoch =196 -- train_error = 0.1794 -- valid_error = 0.1813 -- time = 19.053\n",
      "epoch =197 -- train_error = 0.1789 -- valid_error = 0.1819 -- time = 17.147\n",
      "epoch =198 -- train_error = 0.1791 -- valid_error = 0.1821 -- time = 19.942\n",
      "epoch =199 -- train_error = 0.1794 -- valid_error = 0.1829 -- time = 19.769\n",
      "epoch =200 -- train_error = 0.1801 -- valid_error = 0.1849 -- time = 19.799\n",
      "epoch =201 -- train_error = 0.1799 -- valid_error = 0.1815 -- time = 18.193\n",
      "epoch =202 -- train_error = 0.1793 -- valid_error = 0.1820 -- time = 18.625\n",
      "epoch =203 -- train_error = 0.1794 -- valid_error = 0.1832 -- time = 20.577\n",
      "epoch =204 -- train_error = 0.1792 -- valid_error = 0.1813 -- time = 19.204\n",
      "epoch =205 -- train_error = 0.1798 -- valid_error = 0.1809 -- time = 17.845\n",
      "epoch =206 -- train_error = 0.1788 -- valid_error = 0.1821 -- time = 18.952\n",
      "epoch =207 -- train_error = 0.1786 -- valid_error = 0.1818 -- time = 18.858\n",
      "epoch =208 -- train_error = 0.1782 -- valid_error = 0.1810 -- time = 19.156\n",
      "epoch =209 -- train_error = 0.1788 -- valid_error = 0.1806 -- time = 18.461\n",
      "epoch =210 -- train_error = 0.1793 -- valid_error = 0.1827 -- time = 19.643\n",
      "epoch =211 -- train_error = 0.1793 -- valid_error = 0.1816 -- time = 18.084\n",
      "epoch =212 -- train_error = 0.1788 -- valid_error = 0.1808 -- time = 16.340\n",
      "epoch =213 -- train_error = 0.1783 -- valid_error = 0.1807 -- time = 17.799\n",
      "epoch =214 -- train_error = 0.1785 -- valid_error = 0.1812 -- time = 17.703\n",
      "epoch =215 -- train_error = 0.1794 -- valid_error = 0.1804 -- time = 18.733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =216 -- train_error = 0.1802 -- valid_error = 0.1842 -- time = 18.441\n",
      "epoch =217 -- train_error = 0.1791 -- valid_error = 0.1806 -- time = 19.527\n",
      "epoch =218 -- train_error = 0.1781 -- valid_error = 0.1821 -- time = 18.851\n",
      "epoch =219 -- train_error = 0.1784 -- valid_error = 0.1804 -- time = 19.542\n",
      "epoch =220 -- train_error = 0.1789 -- valid_error = 0.1831 -- time = 19.222\n",
      "epoch =221 -- train_error = 0.1790 -- valid_error = 0.1824 -- time = 18.853\n",
      "epoch =222 -- train_error = 0.1787 -- valid_error = 0.1818 -- time = 17.822\n",
      "epoch =223 -- train_error = 0.1784 -- valid_error = 0.1816 -- time = 17.504\n",
      "epoch =224 -- train_error = 0.1781 -- valid_error = 0.1822 -- time = 18.685\n",
      "epoch =225 -- train_error = 0.1779 -- valid_error = 0.1808 -- time = 18.228\n",
      "epoch =226 -- train_error = 0.1777 -- valid_error = 0.1826 -- time = 17.514\n",
      "epoch =227 -- train_error = 0.1789 -- valid_error = 0.1830 -- time = 19.493\n",
      "epoch =228 -- train_error = 0.1789 -- valid_error = 0.1814 -- time = 20.057\n",
      "epoch =229 -- train_error = 0.1780 -- valid_error = 0.1819 -- time = 18.453\n",
      "epoch =230 -- train_error = 0.1779 -- valid_error = 0.1810 -- time = 17.681\n",
      "epoch =231 -- train_error = 0.1783 -- valid_error = 0.1818 -- time = 18.182\n",
      "epoch =232 -- train_error = 0.1785 -- valid_error = 0.1820 -- time = 19.817\n",
      "epoch =233 -- train_error = 0.1783 -- valid_error = 0.1825 -- time = 17.170\n",
      "epoch =234 -- train_error = 0.1785 -- valid_error = 0.1822 -- time = 18.858\n",
      "epoch =235 -- train_error = 0.1779 -- valid_error = 0.1812 -- time = 17.385\n",
      "epoch =236 -- train_error = 0.1780 -- valid_error = 0.1807 -- time = 18.242\n",
      "epoch =237 -- train_error = 0.1778 -- valid_error = 0.1815 -- time = 19.303\n",
      "epoch =238 -- train_error = 0.1781 -- valid_error = 0.1805 -- time = 18.944\n",
      "epoch =239 -- train_error = 0.1775 -- valid_error = 0.1808 -- time = 18.496\n",
      "epoch =240 -- train_error = 0.1771 -- valid_error = 0.1812 -- time = 17.874\n",
      "epoch =241 -- train_error = 0.1787 -- valid_error = 0.1834 -- time = 18.127\n",
      "epoch =242 -- train_error = 0.1790 -- valid_error = 0.1806 -- time = 17.904\n",
      "epoch =243 -- train_error = 0.1776 -- valid_error = 0.1804 -- time = 18.733\n",
      "epoch =244 -- train_error = 0.1771 -- valid_error = 0.1805 -- time = 18.555\n",
      "epoch =245 -- train_error = 0.1770 -- valid_error = 0.1807 -- time = 18.787\n",
      "epoch =246 -- train_error = 0.1771 -- valid_error = 0.1811 -- time = 19.319\n",
      "epoch =247 -- train_error = 0.1774 -- valid_error = 0.1821 -- time = 18.398\n",
      "epoch =248 -- train_error = 0.1775 -- valid_error = 0.1811 -- time = 18.232\n",
      "epoch =249 -- train_error = 0.1776 -- valid_error = 0.1819 -- time = 18.232\n",
      "epoch =250 -- train_error = 0.1773 -- valid_error = 0.1812 -- time = 18.677\n",
      "epoch =251 -- train_error = 0.1777 -- valid_error = 0.1814 -- time = 18.602\n",
      "epoch =252 -- train_error = 0.1786 -- valid_error = 0.1817 -- time = 18.230\n",
      "epoch =253 -- train_error = 0.1781 -- valid_error = 0.1801 -- time = 18.750\n",
      "epoch =254 -- train_error = 0.1772 -- valid_error = 0.1814 -- time = 20.349\n",
      "epoch =255 -- train_error = 0.1768 -- valid_error = 0.1800 -- time = 17.197\n",
      "epoch =256 -- train_error = 0.1775 -- valid_error = 0.1826 -- time = 18.176\n",
      "epoch =257 -- train_error = 0.1776 -- valid_error = 0.1817 -- time = 17.752\n",
      "epoch =258 -- train_error = 0.1768 -- valid_error = 0.1812 -- time = 18.547\n",
      "epoch =259 -- train_error = 0.1770 -- valid_error = 0.1824 -- time = 18.020\n",
      "epoch =260 -- train_error = 0.1769 -- valid_error = 0.1809 -- time = 19.111\n",
      "epoch =261 -- train_error = 0.1772 -- valid_error = 0.1827 -- time = 18.980\n",
      "epoch =262 -- train_error = 0.1779 -- valid_error = 0.1812 -- time = 19.217\n",
      "epoch =263 -- train_error = 0.1777 -- valid_error = 0.1807 -- time = 18.561\n",
      "epoch =264 -- train_error = 0.1772 -- valid_error = 0.1802 -- time = 19.550\n",
      "epoch =265 -- train_error = 0.1765 -- valid_error = 0.1797 -- time = 18.633\n",
      "epoch =266 -- train_error = 0.1767 -- valid_error = 0.1807 -- time = 17.917\n",
      "epoch =267 -- train_error = 0.1764 -- valid_error = 0.1805 -- time = 17.924\n",
      "epoch =268 -- train_error = 0.1761 -- valid_error = 0.1807 -- time = 17.743\n",
      "epoch =269 -- train_error = 0.1766 -- valid_error = 0.1813 -- time = 19.634\n",
      "epoch =270 -- train_error = 0.1770 -- valid_error = 0.1832 -- time = 18.780\n",
      "epoch =271 -- train_error = 0.1776 -- valid_error = 0.1807 -- time = 19.637\n",
      "epoch =272 -- train_error = 0.1777 -- valid_error = 0.1814 -- time = 16.993\n",
      "epoch =273 -- train_error = 0.1771 -- valid_error = 0.1818 -- time = 18.209\n",
      "epoch =274 -- train_error = 0.1770 -- valid_error = 0.1828 -- time = 19.588\n",
      "epoch =275 -- train_error = 0.1769 -- valid_error = 0.1809 -- time = 19.067\n",
      "epoch =276 -- train_error = 0.1760 -- valid_error = 0.1803 -- time = 20.685\n",
      "epoch =277 -- train_error = 0.1765 -- valid_error = 0.1805 -- time = 20.011\n",
      "epoch =278 -- train_error = 0.1764 -- valid_error = 0.1802 -- time = 18.223\n",
      "epoch =279 -- train_error = 0.1762 -- valid_error = 0.1798 -- time = 18.881\n",
      "epoch =280 -- train_error = 0.1763 -- valid_error = 0.1796 -- time = 19.034\n",
      "epoch =281 -- train_error = 0.1762 -- valid_error = 0.1804 -- time = 18.570\n",
      "epoch =282 -- train_error = 0.1768 -- valid_error = 0.1830 -- time = 19.074\n",
      "epoch =283 -- train_error = 0.1769 -- valid_error = 0.1829 -- time = 19.637\n",
      "epoch =284 -- train_error = 0.1768 -- valid_error = 0.1802 -- time = 18.772\n",
      "epoch =285 -- train_error = 0.1766 -- valid_error = 0.1811 -- time = 18.221\n",
      "epoch =286 -- train_error = 0.1771 -- valid_error = 0.1798 -- time = 17.529\n",
      "epoch =287 -- train_error = 0.1769 -- valid_error = 0.1794 -- time = 19.250\n",
      "epoch =288 -- train_error = 0.1759 -- valid_error = 0.1797 -- time = 16.375\n",
      "epoch =289 -- train_error = 0.1761 -- valid_error = 0.1797 -- time = 17.955\n",
      "epoch =290 -- train_error = 0.1759 -- valid_error = 0.1794 -- time = 17.458\n",
      "epoch =291 -- train_error = 0.1755 -- valid_error = 0.1802 -- time = 18.672\n",
      "epoch =292 -- train_error = 0.1757 -- valid_error = 0.1798 -- time = 17.687\n",
      "epoch =293 -- train_error = 0.1759 -- valid_error = 0.1813 -- time = 17.854\n",
      "epoch =294 -- train_error = 0.1764 -- valid_error = 0.1815 -- time = 18.050\n",
      "epoch =295 -- train_error = 0.1759 -- valid_error = 0.1792 -- time = 19.432\n",
      "epoch =296 -- train_error = 0.1757 -- valid_error = 0.1804 -- time = 18.534\n",
      "epoch =297 -- train_error = 0.1763 -- valid_error = 0.1812 -- time = 17.850\n",
      "epoch =298 -- train_error = 0.1776 -- valid_error = 0.1813 -- time = 18.718\n",
      "epoch =299 -- train_error = 0.1775 -- valid_error = 0.1795 -- time = 18.909\n",
      "epoch =300 -- train_error = 0.1756 -- valid_error = 0.1795 -- time = 17.889\n",
      "epoch =301 -- train_error = 0.1753 -- valid_error = 0.1804 -- time = 17.579\n",
      "epoch =302 -- train_error = 0.1754 -- valid_error = 0.1797 -- time = 18.020\n",
      "epoch =303 -- train_error = 0.1757 -- valid_error = 0.1810 -- time = 18.824\n",
      "epoch =304 -- train_error = 0.1767 -- valid_error = 0.1807 -- time = 19.472\n",
      "epoch =305 -- train_error = 0.1766 -- valid_error = 0.1801 -- time = 18.674\n",
      "epoch =306 -- train_error = 0.1756 -- valid_error = 0.1806 -- time = 20.312\n",
      "epoch =307 -- train_error = 0.1752 -- valid_error = 0.1794 -- time = 18.422\n",
      "epoch =308 -- train_error = 0.1749 -- valid_error = 0.1797 -- time = 19.522\n",
      "epoch =309 -- train_error = 0.1750 -- valid_error = 0.1810 -- time = 18.516\n",
      "epoch =310 -- train_error = 0.1769 -- valid_error = 0.1819 -- time = 18.922\n",
      "epoch =311 -- train_error = 0.1769 -- valid_error = 0.1802 -- time = 18.127\n",
      "epoch =312 -- train_error = 0.1763 -- valid_error = 0.1806 -- time = 18.194\n",
      "epoch =313 -- train_error = 0.1752 -- valid_error = 0.1798 -- time = 16.684\n",
      "epoch =314 -- train_error = 0.1748 -- valid_error = 0.1801 -- time = 17.563\n",
      "epoch =315 -- train_error = 0.1752 -- valid_error = 0.1801 -- time = 18.640\n",
      "epoch =316 -- train_error = 0.1753 -- valid_error = 0.1799 -- time = 18.980\n",
      "epoch =317 -- train_error = 0.1751 -- valid_error = 0.1805 -- time = 18.786\n",
      "epoch =318 -- train_error = 0.1760 -- valid_error = 0.1792 -- time = 18.829\n",
      "epoch =319 -- train_error = 0.1758 -- valid_error = 0.1804 -- time = 18.344\n",
      "epoch =320 -- train_error = 0.1754 -- valid_error = 0.1801 -- time = 16.858\n",
      "epoch =321 -- train_error = 0.1757 -- valid_error = 0.1820 -- time = 19.396\n",
      "epoch =322 -- train_error = 0.1759 -- valid_error = 0.1811 -- time = 18.008\n",
      "epoch =323 -- train_error = 0.1760 -- valid_error = 0.1791 -- time = 18.098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =324 -- train_error = 0.1758 -- valid_error = 0.1809 -- time = 17.983\n",
      "epoch =325 -- train_error = 0.1752 -- valid_error = 0.1813 -- time = 18.183\n",
      "epoch =326 -- train_error = 0.1749 -- valid_error = 0.1805 -- time = 17.236\n",
      "epoch =327 -- train_error = 0.1748 -- valid_error = 0.1800 -- time = 18.435\n",
      "epoch =328 -- train_error = 0.1750 -- valid_error = 0.1790 -- time = 18.732\n",
      "epoch =329 -- train_error = 0.1750 -- valid_error = 0.1805 -- time = 17.859\n",
      "epoch =330 -- train_error = 0.1758 -- valid_error = 0.1798 -- time = 18.909\n",
      "epoch =331 -- train_error = 0.1755 -- valid_error = 0.1818 -- time = 18.888\n",
      "epoch =332 -- train_error = 0.1753 -- valid_error = 0.1815 -- time = 18.885\n",
      "epoch =333 -- train_error = 0.1746 -- valid_error = 0.1800 -- time = 18.953\n",
      "epoch =334 -- train_error = 0.1747 -- valid_error = 0.1799 -- time = 16.832\n",
      "epoch =335 -- train_error = 0.1753 -- valid_error = 0.1801 -- time = 18.653\n",
      "epoch =336 -- train_error = 0.1755 -- valid_error = 0.1801 -- time = 20.624\n",
      "epoch =337 -- train_error = 0.1752 -- valid_error = 0.1808 -- time = 18.844\n",
      "epoch =338 -- train_error = 0.1754 -- valid_error = 0.1807 -- time = 16.951\n",
      "epoch =339 -- train_error = 0.1749 -- valid_error = 0.1805 -- time = 19.950\n",
      "epoch =340 -- train_error = 0.1747 -- valid_error = 0.1810 -- time = 18.393\n",
      "epoch =341 -- train_error = 0.1749 -- valid_error = 0.1802 -- time = 18.455\n",
      "epoch =342 -- train_error = 0.1751 -- valid_error = 0.1806 -- time = 21.053\n",
      "epoch =343 -- train_error = 0.1754 -- valid_error = 0.1819 -- time = 17.935\n",
      "epoch =344 -- train_error = 0.1756 -- valid_error = 0.1802 -- time = 19.175\n",
      "epoch =345 -- train_error = 0.1747 -- valid_error = 0.1804 -- time = 16.792\n",
      "epoch =346 -- train_error = 0.1750 -- valid_error = 0.1814 -- time = 17.915\n",
      "epoch =347 -- train_error = 0.1753 -- valid_error = 0.1815 -- time = 17.007\n",
      "epoch =348 -- train_error = 0.1752 -- valid_error = 0.1794 -- time = 20.572\n",
      "epoch =349 -- train_error = 0.1747 -- valid_error = 0.1799 -- time = 18.876\n",
      "epoch =350 -- train_error = 0.1743 -- valid_error = 0.1797 -- time = 17.671\n",
      "epoch =351 -- train_error = 0.1745 -- valid_error = 0.1805 -- time = 19.344\n",
      "epoch =352 -- train_error = 0.1750 -- valid_error = 0.1817 -- time = 18.813\n",
      "epoch =353 -- train_error = 0.1750 -- valid_error = 0.1809 -- time = 16.817\n",
      "epoch =354 -- train_error = 0.1747 -- valid_error = 0.1803 -- time = 19.689\n",
      "epoch =355 -- train_error = 0.1743 -- valid_error = 0.1792 -- time = 16.728\n",
      "epoch =356 -- train_error = 0.1740 -- valid_error = 0.1793 -- time = 19.060\n",
      "epoch =357 -- train_error = 0.1739 -- valid_error = 0.1805 -- time = 18.256\n",
      "epoch =358 -- train_error = 0.1744 -- valid_error = 0.1796 -- time = 18.427\n",
      "epoch =359 -- train_error = 0.1744 -- valid_error = 0.1792 -- time = 18.412\n",
      "epoch =360 -- train_error = 0.1746 -- valid_error = 0.1810 -- time = 18.025\n",
      "epoch =361 -- train_error = 0.1757 -- valid_error = 0.1803 -- time = 17.932\n",
      "epoch =362 -- train_error = 0.1760 -- valid_error = 0.1806 -- time = 19.017\n",
      "epoch =363 -- train_error = 0.1747 -- valid_error = 0.1792 -- time = 17.758\n",
      "epoch =364 -- train_error = 0.1743 -- valid_error = 0.1796 -- time = 18.070\n",
      "epoch =365 -- train_error = 0.1742 -- valid_error = 0.1801 -- time = 18.917\n",
      "epoch =366 -- train_error = 0.1739 -- valid_error = 0.1803 -- time = 17.739\n",
      "epoch =367 -- train_error = 0.1740 -- valid_error = 0.1805 -- time = 18.090\n",
      "epoch =368 -- train_error = 0.1744 -- valid_error = 0.1820 -- time = 19.301\n",
      "epoch =369 -- train_error = 0.1747 -- valid_error = 0.1807 -- time = 17.584\n",
      "epoch =370 -- train_error = 0.1745 -- valid_error = 0.1799 -- time = 20.151\n",
      "epoch =371 -- train_error = 0.1739 -- valid_error = 0.1800 -- time = 18.120\n",
      "epoch =372 -- train_error = 0.1734 -- valid_error = 0.1805 -- time = 19.318\n",
      "epoch =373 -- train_error = 0.1739 -- valid_error = 0.1808 -- time = 20.263\n",
      "epoch =374 -- train_error = 0.1748 -- valid_error = 0.1829 -- time = 17.475\n",
      "epoch =375 -- train_error = 0.1753 -- valid_error = 0.1828 -- time = 17.604\n",
      "epoch =376 -- train_error = 0.1749 -- valid_error = 0.1802 -- time = 17.907\n",
      "epoch =377 -- train_error = 0.1748 -- valid_error = 0.1787 -- time = 18.789\n",
      "epoch =378 -- train_error = 0.1741 -- valid_error = 0.1796 -- time = 20.502\n",
      "epoch =379 -- train_error = 0.1737 -- valid_error = 0.1809 -- time = 17.926\n",
      "epoch =380 -- train_error = 0.1736 -- valid_error = 0.1808 -- time = 17.591\n",
      "epoch =381 -- train_error = 0.1733 -- valid_error = 0.1797 -- time = 16.715\n",
      "epoch =382 -- train_error = 0.1734 -- valid_error = 0.1798 -- time = 18.373\n",
      "epoch =383 -- train_error = 0.1741 -- valid_error = 0.1833 -- time = 17.941\n",
      "epoch =384 -- train_error = 0.1757 -- valid_error = 0.1825 -- time = 18.903\n",
      "epoch =385 -- train_error = 0.1754 -- valid_error = 0.1825 -- time = 20.102\n",
      "epoch =386 -- train_error = 0.1742 -- valid_error = 0.1820 -- time = 19.410\n",
      "epoch =387 -- train_error = 0.1736 -- valid_error = 0.1802 -- time = 17.128\n",
      "epoch =388 -- train_error = 0.1733 -- valid_error = 0.1799 -- time = 17.877\n",
      "epoch =389 -- train_error = 0.1737 -- valid_error = 0.1804 -- time = 16.925\n",
      "epoch =390 -- train_error = 0.1741 -- valid_error = 0.1799 -- time = 17.000\n",
      "epoch =391 -- train_error = 0.1742 -- valid_error = 0.1811 -- time = 18.273\n",
      "epoch =392 -- train_error = 0.1748 -- valid_error = 0.1811 -- time = 17.667\n",
      "epoch =393 -- train_error = 0.1743 -- valid_error = 0.1807 -- time = 17.990\n",
      "epoch =394 -- train_error = 0.1736 -- valid_error = 0.1808 -- time = 17.803\n",
      "epoch =395 -- train_error = 0.1735 -- valid_error = 0.1796 -- time = 17.541\n",
      "epoch =396 -- train_error = 0.1736 -- valid_error = 0.1803 -- time = 20.833\n",
      "epoch =397 -- train_error = 0.1734 -- valid_error = 0.1802 -- time = 17.197\n",
      "epoch =398 -- train_error = 0.1735 -- valid_error = 0.1809 -- time = 18.259\n",
      "epoch =399 -- train_error = 0.1742 -- valid_error = 0.1793 -- time = 17.070\n",
      "epoch =400 -- train_error = 0.1740 -- valid_error = 0.1799 -- time = 17.609\n",
      "epoch =401 -- train_error = 0.1738 -- valid_error = 0.1788 -- time = 19.225\n",
      "epoch =402 -- train_error = 0.1732 -- valid_error = 0.1790 -- time = 19.354\n",
      "epoch =403 -- train_error = 0.1728 -- valid_error = 0.1789 -- time = 19.303\n",
      "epoch =404 -- train_error = 0.1733 -- valid_error = 0.1796 -- time = 19.993\n",
      "epoch =405 -- train_error = 0.1747 -- valid_error = 0.1805 -- time = 19.182\n",
      "epoch =406 -- train_error = 0.1758 -- valid_error = 0.1823 -- time = 16.490\n",
      "epoch =407 -- train_error = 0.1744 -- valid_error = 0.1796 -- time = 20.102\n",
      "epoch =408 -- train_error = 0.1731 -- valid_error = 0.1799 -- time = 17.278\n",
      "epoch =409 -- train_error = 0.1728 -- valid_error = 0.1795 -- time = 18.362\n",
      "epoch =410 -- train_error = 0.1733 -- valid_error = 0.1811 -- time = 18.992\n",
      "epoch =411 -- train_error = 0.1743 -- valid_error = 0.1803 -- time = 16.652\n",
      "epoch =412 -- train_error = 0.1739 -- valid_error = 0.1799 -- time = 18.142\n",
      "epoch =413 -- train_error = 0.1732 -- valid_error = 0.1799 -- time = 19.414\n",
      "epoch =414 -- train_error = 0.1731 -- valid_error = 0.1805 -- time = 18.779\n",
      "epoch =415 -- train_error = 0.1732 -- valid_error = 0.1797 -- time = 18.754\n",
      "epoch =416 -- train_error = 0.1731 -- valid_error = 0.1806 -- time = 19.615\n",
      "epoch =417 -- train_error = 0.1738 -- valid_error = 0.1799 -- time = 17.684\n",
      "epoch =418 -- train_error = 0.1748 -- valid_error = 0.1811 -- time = 18.400\n",
      "epoch =419 -- train_error = 0.1740 -- valid_error = 0.1798 -- time = 18.224\n",
      "epoch =420 -- train_error = 0.1735 -- valid_error = 0.1796 -- time = 16.425\n",
      "epoch =421 -- train_error = 0.1729 -- valid_error = 0.1795 -- time = 17.199\n",
      "epoch =422 -- train_error = 0.1726 -- valid_error = 0.1795 -- time = 18.881\n",
      "epoch =423 -- train_error = 0.1734 -- valid_error = 0.1811 -- time = 17.951\n",
      "epoch =424 -- train_error = 0.1741 -- valid_error = 0.1805 -- time = 19.137\n",
      "epoch =425 -- train_error = 0.1740 -- valid_error = 0.1788 -- time = 17.674\n",
      "epoch =426 -- train_error = 0.1735 -- valid_error = 0.1793 -- time = 19.610\n",
      "epoch =427 -- train_error = 0.1735 -- valid_error = 0.1794 -- time = 19.190\n",
      "epoch =428 -- train_error = 0.1734 -- valid_error = 0.1800 -- time = 19.254\n",
      "epoch =429 -- train_error = 0.1728 -- valid_error = 0.1790 -- time = 17.453\n",
      "epoch =430 -- train_error = 0.1726 -- valid_error = 0.1791 -- time = 17.376\n",
      "epoch =431 -- train_error = 0.1732 -- valid_error = 0.1812 -- time = 21.338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =432 -- train_error = 0.1743 -- valid_error = 0.1798 -- time = 18.873\n",
      "epoch =433 -- train_error = 0.1745 -- valid_error = 0.1790 -- time = 17.942\n",
      "epoch =434 -- train_error = 0.1733 -- valid_error = 0.1800 -- time = 18.396\n",
      "epoch =435 -- train_error = 0.1729 -- valid_error = 0.1791 -- time = 19.433\n",
      "epoch =436 -- train_error = 0.1724 -- valid_error = 0.1791 -- time = 18.786\n",
      "epoch =437 -- train_error = 0.1723 -- valid_error = 0.1788 -- time = 16.956\n",
      "epoch =438 -- train_error = 0.1724 -- valid_error = 0.1793 -- time = 19.665\n",
      "epoch =439 -- train_error = 0.1733 -- valid_error = 0.1815 -- time = 18.663\n",
      "epoch =440 -- train_error = 0.1742 -- valid_error = 0.1799 -- time = 16.251\n",
      "epoch =441 -- train_error = 0.1740 -- valid_error = 0.1794 -- time = 11.907\n",
      "epoch =442 -- train_error = 0.1729 -- valid_error = 0.1804 -- time = 12.444\n",
      "epoch =443 -- train_error = 0.1726 -- valid_error = 0.1804 -- time = 12.154\n",
      "epoch =444 -- train_error = 0.1727 -- valid_error = 0.1800 -- time = 12.122\n",
      "epoch =445 -- train_error = 0.1734 -- valid_error = 0.1813 -- time = 12.060\n",
      "epoch =446 -- train_error = 0.1733 -- valid_error = 0.1810 -- time = 12.128\n",
      "epoch =447 -- train_error = 0.1729 -- valid_error = 0.1797 -- time = 12.122\n",
      "epoch =448 -- train_error = 0.1726 -- valid_error = 0.1793 -- time = 12.169\n",
      "epoch =449 -- train_error = 0.1726 -- valid_error = 0.1806 -- time = 12.069\n",
      "epoch =450 -- train_error = 0.1729 -- valid_error = 0.1806 -- time = 9.499\n",
      "epoch =451 -- train_error = 0.1733 -- valid_error = 0.1798 -- time = 8.514\n",
      "epoch =452 -- train_error = 0.1736 -- valid_error = 0.1798 -- time = 8.480\n",
      "epoch =453 -- train_error = 0.1737 -- valid_error = 0.1799 -- time = 8.474\n",
      "epoch =454 -- train_error = 0.1731 -- valid_error = 0.1807 -- time = 8.478\n",
      "epoch =455 -- train_error = 0.1723 -- valid_error = 0.1797 -- time = 8.475\n",
      "epoch =456 -- train_error = 0.1721 -- valid_error = 0.1792 -- time = 8.474\n",
      "epoch =457 -- train_error = 0.1722 -- valid_error = 0.1803 -- time = 8.473\n",
      "epoch =458 -- train_error = 0.1730 -- valid_error = 0.1798 -- time = 8.473\n",
      "epoch =459 -- train_error = 0.1739 -- valid_error = 0.1821 -- time = 8.473\n",
      "epoch =460 -- train_error = 0.1735 -- valid_error = 0.1821 -- time = 8.475\n",
      "epoch =461 -- train_error = 0.1724 -- valid_error = 0.1805 -- time = 8.473\n",
      "epoch =462 -- train_error = 0.1719 -- valid_error = 0.1800 -- time = 8.474\n",
      "epoch =463 -- train_error = 0.1720 -- valid_error = 0.1806 -- time = 8.486\n",
      "epoch =464 -- train_error = 0.1722 -- valid_error = 0.1806 -- time = 8.474\n",
      "epoch =465 -- train_error = 0.1731 -- valid_error = 0.1816 -- time = 8.474\n",
      "epoch =466 -- train_error = 0.1737 -- valid_error = 0.1803 -- time = 8.478\n",
      "epoch =467 -- train_error = 0.1743 -- valid_error = 0.1793 -- time = 8.471\n",
      "epoch =468 -- train_error = 0.1730 -- valid_error = 0.1795 -- time = 8.476\n",
      "epoch =469 -- train_error = 0.1722 -- valid_error = 0.1807 -- time = 8.470\n",
      "epoch =470 -- train_error = 0.1725 -- valid_error = 0.1804 -- time = 8.476\n",
      "epoch =471 -- train_error = 0.1718 -- valid_error = 0.1795 -- time = 8.475\n",
      "epoch =472 -- train_error = 0.1723 -- valid_error = 0.1815 -- time = 8.475\n",
      "epoch =473 -- train_error = 0.1732 -- valid_error = 0.1811 -- time = 8.472\n",
      "epoch =474 -- train_error = 0.1736 -- valid_error = 0.1827 -- time = 8.473\n",
      "epoch =475 -- train_error = 0.1733 -- valid_error = 0.1794 -- time = 8.484\n",
      "epoch =476 -- train_error = 0.1727 -- valid_error = 0.1790 -- time = 8.481\n",
      "epoch =477 -- train_error = 0.1719 -- valid_error = 0.1797 -- time = 8.736\n",
      "epoch =478 -- train_error = 0.1717 -- valid_error = 0.1787 -- time = 8.513\n",
      "epoch =479 -- train_error = 0.1724 -- valid_error = 0.1813 -- time = 8.477\n",
      "epoch =480 -- train_error = 0.1734 -- valid_error = 0.1805 -- time = 8.474\n",
      "epoch =481 -- train_error = 0.1733 -- valid_error = 0.1795 -- time = 8.471\n",
      "epoch =482 -- train_error = 0.1725 -- valid_error = 0.1798 -- time = 8.475\n",
      "epoch =483 -- train_error = 0.1722 -- valid_error = 0.1800 -- time = 8.478\n",
      "epoch =484 -- train_error = 0.1722 -- valid_error = 0.1801 -- time = 8.471\n",
      "epoch =485 -- train_error = 0.1724 -- valid_error = 0.1803 -- time = 8.549\n",
      "epoch =486 -- train_error = 0.1727 -- valid_error = 0.1808 -- time = 8.472\n",
      "epoch =487 -- train_error = 0.1730 -- valid_error = 0.1818 -- time = 8.485\n",
      "epoch =488 -- train_error = 0.1724 -- valid_error = 0.1806 -- time = 8.474\n",
      "epoch =489 -- train_error = 0.1718 -- valid_error = 0.1796 -- time = 8.471\n",
      "epoch =490 -- train_error = 0.1724 -- valid_error = 0.1808 -- time = 8.473\n",
      "epoch =491 -- train_error = 0.1731 -- valid_error = 0.1798 -- time = 8.486\n",
      "epoch =492 -- train_error = 0.1728 -- valid_error = 0.1799 -- time = 8.473\n",
      "epoch =493 -- train_error = 0.1728 -- valid_error = 0.1805 -- time = 8.470\n",
      "epoch =494 -- train_error = 0.1725 -- valid_error = 0.1803 -- time = 8.473\n",
      "epoch =495 -- train_error = 0.1721 -- valid_error = 0.1794 -- time = 8.470\n",
      "epoch =496 -- train_error = 0.1717 -- valid_error = 0.1796 -- time = 8.474\n",
      "epoch =497 -- train_error = 0.1717 -- valid_error = 0.1788 -- time = 8.478\n",
      "epoch =498 -- train_error = 0.1716 -- valid_error = 0.1797 -- time = 8.470\n",
      "epoch =499 -- train_error = 0.1722 -- valid_error = 0.1795 -- time = 8.466\n"
     ]
    }
   ],
   "source": [
    "mb_size = 500 # minibatch size\n",
    "epochs = 500\n",
    "lrate = 0.0005\n",
    "train_errors = []\n",
    "valid_errors = []\n",
    "train_start_time = time.time()\n",
    "\n",
    "train_size = len(train_y.eval()) # number of training examples \n",
    "training_idxs = np.arange(train_size)\n",
    "\n",
    "print(\"***** training started  *****\")\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    rng_np.shuffle(training_idxs) # randomize training idxs, randomizes in-place\n",
    "    try:    \n",
    "        t_minib_errors = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_startidx in range(0, train_size, mb_size):\n",
    "            mini_batch = training_idxs[batch_startidx:batch_startidx+mb_size]\n",
    "            \n",
    "            ## MAIN TRAINING\n",
    "            rec_error, rec, low_dim = autoencoder_train(\n",
    "                    mbidxs = np.asarray(mini_batch).astype(np.int32),\n",
    "                    learning_rate=lrate)\n",
    "            t_minib_errors.append(rec_error)\n",
    "\n",
    "        valid_rec_error, valid_rec, low_dim = autoencoder_predict(input = valid_x.get_value())\n",
    "        train_errors.append(np.mean(t_minib_errors))\n",
    "        valid_errors.append(np.mean(valid_rec_error))\n",
    "        print(\"epoch ={:>3d} -- train_error = {:>6.4f} -- valid_error = {:>6.4f} -- time = {:>5.3f}\".format(epoch, train_errors[-1], valid_errors[-1], time.time()-start_time))    \n",
    "        \n",
    "    except KeyboardInterrupt as e:\n",
    "        print(\"***** stopping *****\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2f5cb1cd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(train_errors)), train_errors, \"r\", label=\"Train\")\n",
    "plt.plot(range(len(valid_errors)), valid_errors, \"g\", label=\"Valid\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_dict = {}\n",
    "for par in params:\n",
    "    param_dict.update({par.name:par.get_value()})\n",
    "\n",
    "f = open('autoencoder.save', 'wb')\n",
    "pickle.dump(param_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
