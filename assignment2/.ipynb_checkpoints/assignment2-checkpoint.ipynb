{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Extend and improve the accuracy of the Convolution Neural Network discussed in the demonstration by doing the following things: </p>\n",
    "<ul>\n",
    "<li>Creating a deep Convolutional Neural Network.</li>\n",
    "<li>Tweaking the hyper-parameters such as the number of channels in each layer, activation functions, mini batch size, etc;</li>\n",
    "</ul>\n",
    "<p>The architecture of the network that we will create is similar to the figure below (This figure will not be displayed when exported to .pdf):</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/AjQYgPv.png\" alt=\"Demo Architecture\" title=\"Figure 1\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We will begin by first importing the necessary python libraries:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Import complete *****\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "from exercise_helper import load_data, pooling, convLayer\n",
    "from exercise_helper import fullyConnectedLayer\n",
    "from exercise_helper import negative_log_lik, errors\n",
    "from exercise_helper import generate_plot\n",
    "%matplotlib inline\n",
    "# Setting the random number generator\n",
    "rng = numpy.random.RandomState(23455)\n",
    "rd.seed(23455)\n",
    "print('***** Import complete *****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Note: If the import of 'exercise_helper' fails, ensure that 'exercise_helper.py' is in the same folder as this notebook.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we can create the Theano Computation Graph. We shall partition the data into mini-batches and then create a computation graph for training, validation and testing. The overall logic is very similar to the demonstration. Refer to the demonstration for more details.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_conv_net(learning_rate, num_epochs,\n",
    "                        num_filters, mini_batch_size, activation):\n",
    "    # Function to create the convolutional neural network, train and\n",
    "    # evaluate it. This function must be called to run the network.\n",
    "    \n",
    "    # Inputs:\n",
    "    # learning_rate - Learning rate for Stochastic Gradient Descent\n",
    "    # num_epochs - Number of training epochs\n",
    "    # num_filters - Number of kernels for each convolution layer\n",
    "    #               for e.g. 2 layers - [20, 50]. \n",
    "    #.              layer1 = 20, layer2 = 50\n",
    "    # mini_batch_size - Mini-batch size to be used\n",
    "    # activation - Activation function to use\n",
    "    \n",
    "    # Outputs:\n",
    "    # Plot of the cost, prediction errors on validation set and\n",
    "    # visualisation of weights of the first convolutional layer\n",
    "    \n",
    "    # Partitioning into mini- batches\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "    n_train_batches //= mini_batch_size\n",
    "    n_valid_batches //= mini_batch_size\n",
    "    n_test_batches //= mini_batch_size\n",
    "\n",
    "    print('train: %d batches, test: %d batches,'\n",
    "          ' validation: %d batches'\n",
    "          % (n_train_batches, n_test_batches, n_valid_batches))\n",
    "    \n",
    "    mb_index = T.lscalar() # mini-batch index\n",
    "    x = T.matrix('x') # rasterised images\n",
    "    y = T.ivector('y') # image labels\n",
    "    layer_weights = [];\n",
    "    print('***** Constructing model ***** ')\n",
    "\n",
    "    # Reshaping matrix of mini_batch_size set of images into a \n",
    "    # 4-D tensor \n",
    "    layer0_input = x.reshape((mini_batch_size, 1, 28, 28))\n",
    "\n",
    "    ################# Insert Your Code Here ###################\n",
    "\n",
    "    # Construct first convolution and pooling layer\n",
    "    # Hint: Use the convLayer function. See demonstration.\n",
    "    [layer0_output, layer0_params] = convLayer(\n",
    "        rng,\n",
    "        data_input=layer0_input,\n",
    "        image_spec=(mini_batch_size, 1, 28, 28),\n",
    "        filter_spec=(num_filters[0], 1, 5, 5),\n",
    "        pool_size=(2, 2),\n",
    "        activation=activation)\n",
    "    \n",
    "    # Construct second convolution and pooling layer\n",
    "    # Hint: Use the convLayer function. See demonstration.   \n",
    "    [layer1_output, layer1_params] = convLayer(\n",
    "        rng,\n",
    "        data_input=layer0_output,\n",
    "        image_spec=(mini_batch_size, num_filters[0], 12, 12),\n",
    "        filter_spec=(num_filters[1], num_filters[0], 5, 5),\n",
    "        pool_size=(2, 2),\n",
    "        activation=activation)\n",
    " \n",
    "    # Classify the values using the fully-connected\n",
    "    # activation layer.\n",
    "    # Hint: Remember to flatten the output from the \n",
    "    # convolutional layer. Use the fullyConnectedLayer function.\n",
    "    # See demonstration.   \n",
    "    fc_layer_input = layer1_output.flatten(2)\n",
    "    [p_y_given_x, y_pred, fc_layer_params] = fullyConnectedLayer(\n",
    "        data_input=fc_layer_input,\n",
    "        num_in=num_filters[1]*4*4,\n",
    "        num_out=10)              \n",
    "\n",
    "    ##########################################################\n",
    "\n",
    "    # Cost that is minimised during stochastic descent.\n",
    "    cost = negative_log_lik(y=y, p_y_given_x=p_y_given_x)\n",
    "\n",
    "    # Creating a function that computes the mistakes on the test set\n",
    "    # mb_index is the mini_batch_index\n",
    "    test_model = theano.function(\n",
    "        [mb_index],\n",
    "        errors(y, y_pred),\n",
    "        givens={\n",
    "            x: test_set_x[\n",
    "                mb_index * mini_batch_size:\n",
    "                (mb_index + 1) * mini_batch_size],\n",
    "            y: test_set_y[\n",
    "                mb_index * mini_batch_size:\n",
    "                (mb_index + 1) * mini_batch_size]})\n",
    "\n",
    "    # Creating a function that computes the mistakes on the validation\n",
    "    # set\n",
    "    valid_model = theano.function(\n",
    "        [mb_index],\n",
    "        errors(y, y_pred),\n",
    "        givens={\n",
    "            x: valid_set_x[\n",
    "                mb_index * mini_batch_size:\n",
    "                (mb_index + 1) * mini_batch_size],\n",
    "            y: valid_set_y[\n",
    "                mb_index * mini_batch_size:\n",
    "                (mb_index + 1) * mini_batch_size]})\n",
    "    \n",
    "    ################# Insert Your Code Here ###################\n",
    "\n",
    "    # Create list of parameters to fit during training.\n",
    "    # Hint: Include the parameters from the two convolution layers\n",
    "    # and activation layer\n",
    "    \n",
    "    params = fc_layer_params + layer0_params + layer1_params\n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    # Creating a list of gradients\n",
    "    grads = T.grad(cost, params)\n",
    "\n",
    "    # Creating a function that updates the model parameters by SGD.\n",
    "    # The updates list is created by looping over all \n",
    "    # params[i], grads[i]) pairs.\n",
    "    updates = [(param_i, param_i - learning_rate * grad_i)\n",
    "               for param_i, grad_i in zip(params, grads)]\n",
    "\n",
    "    train_model = theano.function(\n",
    "        [mb_index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[\n",
    "                mb_index * mini_batch_size:\n",
    "                (mb_index + 1) * mini_batch_size],\n",
    "            y: train_set_y[\n",
    "                mb_index * mini_batch_size:\n",
    "                (mb_index + 1) * mini_batch_size]})\n",
    "\n",
    "    epoch = 0\n",
    "    cost_arr = numpy.array([])\n",
    "    valid_score_arr = numpy.array([])\n",
    "    valid_score_arr = numpy.append(valid_score_arr, 1)\n",
    "\n",
    "    print('***** Training model *****')\n",
    "    if (num_epochs < 1):\n",
    "        print(\"Too few epochs!\")\n",
    "        return\n",
    "    while (epoch < num_epochs):\n",
    "        epoch = epoch + 1\n",
    "        print(\"Training in epoch: %d / %d\" % (epoch, num_epochs),\n",
    "              end='\\r')\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            # Computing number of iterations performed or total number\n",
    "            # of mini-batches executed.\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            # cost of each minibatch\n",
    "            cost_ij = train_model(minibatch_index)\n",
    "            cost_arr = numpy.append(cost_arr, cost_ij)\n",
    "\n",
    "        # Computing loss on each validation mini-batch after each epoch\n",
    "        valid_losses = [valid_model(i) for i in range(n_valid_batches)]\n",
    "        valid_score_arr = numpy.append(\n",
    "                                       valid_score_arr,\n",
    "                                        numpy.mean(valid_losses))\n",
    "    print('***** Training Complete *****')\n",
    "    # Computing mean error rate on test set\n",
    "    test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "    test_score = numpy.mean(test_losses)\n",
    "    print('Prediction error: %f %%' % (test_score * 100.))\n",
    "    # Generating the plots\n",
    "    generate_plot(cost_arr, range(1, iter+2),\n",
    "                  valid_score_arr,\n",
    "                  range(0, epoch+1),\n",
    "                  layer0_params[0].get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we shall define some hyper-parameters and evaluate the model. We will compute the final Prediction Error on the test set. To complete this assignment, you must do the following things:</p>\n",
    "<ol>\n",
    "<li>Set your parameters in the cell below</li>\n",
    "<li>Run the experiment. Then, describe and discuss it in the 'Conclusions' cell below. You must do this for each experiment that you run.</li>\n",
    "<li>Especially in CSC notebooks, do not forget to restart the kernel after each experiment.</li>\n",
    "<li>Repeat from step 1 and perform at least 4 experiments.</li>\n",
    "<li>Run the experiment with the best result again to display the plots in the final submission.</li>\n",
    "<li>Download the completed assignment as PDF and submit as usual. The final PDF will contain the plots from only your best experiment as well as the discussion of all your experiments in the 'Conclusions' cell.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Loading data *****\n",
      "Training set: 6000 samples\n",
      "Test set: 2000 samples\n",
      "Validation set: 2000 samples\n",
      "train: 120 batches, test: 40 batches, validation: 40 batches\n",
      "***** Constructing model ***** \n",
      "***** Training model *****\n",
      "Training in epoch: 20 / 30\r"
     ]
    }
   ],
   "source": [
    "################# Insert Your Code Here ###################\n",
    "\n",
    "# Description and examples of parameters to train_test_conv_net()\n",
    "\n",
    "# learning_rate - Sets the learning rate for Stochastic Gradient\n",
    "#                 Descent. e.g.: 0.1\n",
    "# num_epochs - Sets the number of training epochs. E.g.: 10\n",
    "# num_filters - Sets the number of kernels for each \n",
    "#               convolution layer. for e.g. 2 layers: [4, 8]\n",
    "#               implies layer1 = 4, layer2 = 8\n",
    "# mini_batch_size - Sets the mini-batch size to be used in \n",
    "#                   the experiment. E.g: 50\n",
    "# activation - Sets the activation function to be used.\n",
    "#              E.g.: T.tanh\n",
    "# train_size - Sets the number of training samples to be used.\n",
    "#              E.g. 6000. \n",
    "\n",
    "# Note: The Kernel may crash on too large values of the \n",
    "# num_epochs, num_filters, mini_batch_size and train_size due to \n",
    "# memory limitations. This may happen especially on CSC notebooks\n",
    "# as it is a shared resource.\n",
    "# If that happens use smaller values!\n",
    "\n",
    "learning_rate   = 0.1\n",
    "num_epochs      = 30\n",
    "num_filters     = [20,30]\n",
    "mini_batch_size = 50\n",
    "activation      = T.nnet.relu\n",
    "train_size      = 6000\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# Loading dataset\n",
    "# We use only a subset of the full dataset.\n",
    "# Validation and test sets will be 1/3 of train \n",
    "# set size\n",
    "datasets = load_data('mnist.pkl.gz', train_size)\n",
    "train_set_x, train_set_y = datasets[0]\n",
    "valid_set_x, valid_set_y = datasets[1]\n",
    "test_set_x, test_set_y = datasets[2]\n",
    "print('Training set: %d samples'\n",
    "      %(train_set_x.get_value(borrow=True).shape[0])) \n",
    "print('Test set: %d samples'\n",
    "      %(test_set_x.get_value(borrow=True).shape[0]))\n",
    "print('Validation set: %d samples'\n",
    "      %(valid_set_x.get_value(borrow=True).shape[0]))\n",
    "\n",
    "# Beginning the training process\n",
    "train_test_conv_net(learning_rate, num_epochs,\n",
    "                    num_filters, mini_batch_size, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p>Now go ahead and experiment with different hyper-parameters such as different number of filters, different number of convolution layers, activation functions etc;. Try to find out which configuration gives the best accuracy.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each experiment, I focus on manipulating one hyperparameter to test its effect on model accuracy.\n",
    "\n",
    "#### Experiment 1: number of hidden layers\n",
    "learning_rate   = 0.1\n",
    "num_epochs      = 10\n",
    "num_filters     = [9,16]\n",
    "mini_batch_size = 50\n",
    "activation      = T.tanh\n",
    "train_size      = 6000\n",
    "Prediction error: 3.55%\n",
    "\n",
    "The motivation of this experiment is to compare 1 hidden layer CNN and 2 hidden layer CNN. Every hyperparameter is the same as in the Demo, only adding a second layer of 16 filters. Compared to the 5.15% prediction error from the Demo model, adding a second hidden layer reduced the prediction error significantly.\n",
    "\n",
    "#### Experiment 2: number of filters \n",
    "learning_rate   = 0.1\n",
    "num_epochs      = 10\n",
    "num_filters     = [2,3]\n",
    "mini_batch_size = 50\n",
    "activation      = T.tanh\n",
    "train_size      = 6000\n",
    "Prediction error: 7.95%\n",
    "\n",
    "num_filters     = [20,30]\n",
    "Prediction error: 2.6%\n",
    "\n",
    "num_filters     = [40,50]\n",
    "Prediction error: 5.8%\n",
    "\n",
    "Very small number of filters performs worse, due to underfitting. As network complexity increases, the prediction error decreases. Very large number of filters also performs worse, due to overfitting. Increased complexityh takes significantly more time to train the model.\n",
    "\n",
    "#### Experiment 3: activation function\n",
    "learning_rate   = 0.1\n",
    "num_epochs      = 10\n",
    "num_filters     = [2,3]\n",
    "mini_batch_size = 50\n",
    "activation      = T.nnet.relu\n",
    "train_size      = 6000\n",
    "Prediction error: 3%\n",
    "\n",
    "activation      = T.nnet.nnet.sigmoid\n",
    "train_size      = 6000\n",
    "Prediction error: 8.75%\n",
    "\n",
    "Sigmoid activation performs significatnly worse than tanh, rectified linear unit performs better. This could be cause by sigmoid's heavily being affected to vanishing gradient problem when using graident descent to optimize the parameters. Tanh and rectified linear unit's design alleviates the problem, especially relu being linear rather than exponential based.\n",
    "\n",
    "#### Experiment 4: training data size\n",
    "learning_rate   = 0.1\n",
    "num_epochs      = 10\n",
    "num_filters     = [9,16]\n",
    "mini_batch_size = 50\n",
    "activation      = T.tanh\n",
    "train_size      = 3000\n",
    "Prediction error: 4.5%\n",
    "\n",
    "Smaller training set yields larger predictione error as less information is available for the model to learn from, as can be expected.\n",
    "\n",
    "#### Experiment 5: mini batch size\n",
    "learning_rate   = 0.1\n",
    "num_epochs      = 10\n",
    "num_filters     = [9,16]\n",
    "mini_batch_size = 2\n",
    "activation      = T.tanh\n",
    "train_size      = 6000\n",
    "Prediction error: 13.4%\n",
    "\n",
    "mini_batch_size = 5\n",
    "Prediction error: 2.75%\n",
    "\n",
    "mini_batch_size = 100\n",
    "Prediction error: 4.85%\n",
    "\n",
    "mini_batch_size = 400\n",
    "Prediction error: 8.1%\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item The bigger mini batch size is, the less iterations it takes to complete a fixed number of epochs. \n",
    "    \\item Really small mini batch size moves really randomly, not necessarily always towards steepest descent, and causes very high error rate.\n",
    "    \\item A smaller mini batch size can sometimes give better result.\n",
    "    \\item Reasonable mini batch sizes doesn't seem to change the model result drastically (prediction error ranging from 2-5% have been seen when experimenting with mini batch size from 5-400), only the training time varies. \n",
    "    \\item Very large mini batch size requires more epochs to properly reach a local minimum.   \n",
    "\\end{itemize}\n",
    "\n",
    "#### Experiment 6: number of epochs\n",
    "learning_rate   = 0.1\n",
    "num_epochs      = 20\n",
    "num_filters     = [9,16]\n",
    "mini_batch_size = 50\n",
    "activation      = T.tanh\n",
    "train_size      = 3000\n",
    "Prediction error: 2.4%\n",
    "\n",
    "More epochs gives gradient descent more runs to find optimum, therefore gives better result. However after a certain amount of epochs, the marginal gain decreases significantly. The time it takes to train model increases proportionaly.\n",
    "\n",
    "#### Experiment 7: learning rate\n",
    "learning_rate   = 0.5\n",
    "num_epochs      = 10\n",
    "num_filters     = [9,16]\n",
    "mini_batch_size = 50\n",
    "activation      = T.tanh\n",
    "train_size      = 3000\n",
    "Prediction error: 91.35%\n",
    "\n",
    "learning_rate   = 0.05\n",
    "Prediction error: 9.1%\n",
    "\n",
    "Setting learning rate is a trial and error process, too big learning rate, overshooting happens, too small learning rate, the model takes a long time to converge, for example when setting it to 0.05, it didn't converge properly in 10 epochs.\n",
    "\n",
    "### Conclusion:\n",
    "The best model from all the experiments is found to be using the following parameters:\n",
    "learning_rate   = 0.1\n",
    "num_epochs      = 30\n",
    "num_filters     = [20,30]\n",
    "mini_batch_size = 50\n",
    "activation      = T.nnet.relu\n",
    "train_size      = 6000\n",
    "\n",
    "What I found to be most impactful in terms of increasing model accuracy is to change the activation function, using a reasonably biger number of filters and allowing more epochs to find a good optimum. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
